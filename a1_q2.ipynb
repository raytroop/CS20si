{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We got the accuracy of ~90% on our MNIST dataset with our vanilla model, which is\n",
    "unacceptable. The dataset is basically solved and state of the art models reach accuracies above\n",
    "99%. You can use whatever loss functions, optimizers, even models that you want, as long as\n",
    "your model is built in TensorFlow. You can save your code in the file q2.py. In the comments,\n",
    "explain what you decided to do, instruction on how to run your code, and report your results.\n",
    "Iâ€™m happy if you can reach 97%.\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('./data/mnist', one_hot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# Features are of the type float, and labels are of the type int\n",
    "X =  tf.placeholder(dtype= tf.float32, shape=(None, 784), name ='input_X')\n",
    "Y = tf.placeholder(dtype=tf.int16, shape=(None, 10), name = 'output_Y')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name = 'keep_prob')\n",
    "# Step 3: create weights and bias\n",
    "# weights and biases are initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = X * w + b\n",
    "# shape of b depends on Y\n",
    "#w = tf.Variable(tf.zeros([784,10]), name = 'weight')\n",
    "# layer-1\n",
    "w1 = tf.Variable(tf.random_normal(shape=[784, 448], stddev=0.01), name='weights_1')  ## initialize all to zero or nomal  doesnt matter here\n",
    "b1 = tf.Variable(tf.zeros([1, 448]), name ='bias_1')\n",
    "# layer-2\n",
    "w2 = tf.Variable(tf.random_normal(shape=[448, 192], stddev=0.01), name='weights_2')  ## initialize all to zero or nomal  doesnt matter here\n",
    "b2 = tf.Variable(tf.zeros([1, 192]), name ='bias_2')\n",
    "# layer-2\n",
    "w3 = tf.Variable(tf.random_normal(shape=[192, 96], stddev=0.01), name='weights_3')  ## initialize all to zero or nomal  doesnt matter here\n",
    "b3 = tf.Variable(tf.zeros([1, 96]), name ='bias_3')\n",
    "\n",
    "# output-layer\n",
    "w = tf.Variable(tf.random_normal(shape=[96, 10], stddev=0.01), name='weights')  ## initialize all to zero or nomal  doesnt matter here\n",
    "b = tf.Variable(tf.zeros([1, 10]), name ='bias')\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "# to get the probability distribution of possible label of the image\n",
    "# DO NOT DO SOFTMAX HERE\n",
    "X1 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "X1_keep = tf.nn.dropout(X1, keep_prob)\n",
    "\n",
    "X2 = tf.nn.relu(tf.matmul(X1_keep, w2) + b2)\n",
    "X2_keep = tf.nn.dropout(X2, keep_prob)\n",
    "\n",
    "X3 = tf.nn.relu(tf.matmul(X2_keep, w3) + b3)\n",
    "X3_keep = tf.nn.dropout(X3, keep_prob)\n",
    "\n",
    "logits = tf.matmul(X3_keep, w) + b\n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy loss of the real labels with the softmax of logits\n",
    "# use the method:\n",
    "# tf.nn.softmax_cross_entropy_with_logits(logits, Y)\n",
    "# then use tf.reduce_mean to get the mean loss of the batch\n",
    "Y_predicted = tf.argmax(logits, axis= 1)\n",
    "Y_true = tf.argmax(Y, axis= 1)\n",
    "correct_preds = tf.reduce_sum(tf.cast(tf.equal(Y_predicted, Y_true), tf.float32))\n",
    "\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(xentropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent to minimize loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.1543755848831105\n",
      "Average loss epoch 1: 0.48081847607532396\n",
      "Average loss epoch 2: 0.3271671505731957\n",
      "Average loss epoch 3: 0.2524201255813937\n",
      "Average loss epoch 4: 0.21051144063751273\n",
      "Average loss epoch 5: 0.1799540798240733\n",
      "Average loss epoch 6: 0.1631539305933168\n",
      "Average loss epoch 7: 0.14381615441536236\n",
      "Average loss epoch 8: 0.13449134621943268\n",
      "Average loss epoch 9: 0.12073376377887815\n",
      "Average loss epoch 10: 0.1097979530274311\n",
      "Average loss epoch 11: 0.11008031760281492\n",
      "Average loss epoch 12: 0.10197792626986994\n",
      "Average loss epoch 13: 0.0942220403421148\n",
      "Average loss epoch 14: 0.0907497001794454\n",
      "Average loss epoch 15: 0.08573850481031096\n",
      "Average loss epoch 16: 0.08111251448498707\n",
      "Average loss epoch 17: 0.07691869888210966\n",
      "Average loss epoch 18: 0.07091789633453449\n",
      "Average loss epoch 19: 0.07417704614105626\n",
      "Average loss epoch 20: 0.06803944988924766\n",
      "Average loss epoch 21: 0.06785782692936536\n",
      "Average loss epoch 22: 0.06190382381117789\n",
      "Average loss epoch 23: 0.05934349722533583\n",
      "Average loss epoch 24: 0.0591661133846947\n",
      "Average loss epoch 25: 0.05747856377301929\n",
      "Average loss epoch 26: 0.05643983495151885\n",
      "Average loss epoch 27: 0.052244556319212244\n",
      "Average loss epoch 28: 0.05398573357368184\n",
      "Average loss epoch 29: 0.05172469701931298\n",
      "Average loss epoch 30: 0.04982780414485486\n",
      "Average loss epoch 31: 0.05034028834480549\n",
      "Average loss epoch 32: 0.048069389972174276\n",
      "Average loss epoch 33: 0.04519382485615037\n",
      "Average loss epoch 34: 0.04414023817560383\n",
      "Average loss epoch 35: 0.04490002262118821\n",
      "Average loss epoch 36: 0.042489756264279935\n",
      "Average loss epoch 37: 0.04262691348025175\n",
      "Average loss epoch 38: 0.04336995520045824\n",
      "Average loss epoch 39: 0.04012181245173528\n",
      "Average loss epoch 40: 0.04220278094583583\n",
      "Average loss epoch 41: 0.041128465244285414\n",
      "Average loss epoch 42: 0.039538092946059235\n",
      "Average loss epoch 43: 0.038584055796822656\n",
      "Average loss epoch 44: 0.03908744906118818\n",
      "Average loss epoch 45: 0.038923522901381846\n",
      "Average loss epoch 46: 0.03733768605705455\n",
      "Average loss epoch 47: 0.0384827908302579\n",
      "Average loss epoch 48: 0.03860839441523095\n",
      "Average loss epoch 49: 0.034537062971530676\n",
      "Average loss epoch 50: 0.03456827231343383\n",
      "Average loss epoch 51: 0.03618010954287286\n",
      "Average loss epoch 52: 0.03415667225615444\n",
      "Average loss epoch 53: 0.030758734374681365\n",
      "Average loss epoch 54: 0.033651372309008214\n",
      "Average loss epoch 55: 0.03245087145940862\n",
      "Average loss epoch 56: 0.030821017912266966\n",
      "Average loss epoch 57: 0.03332833904842628\n",
      "Average loss epoch 58: 0.02934349578953235\n",
      "Average loss epoch 59: 0.036059828666604565\n",
      "Average loss epoch 60: 0.03127848403093135\n",
      "Average loss epoch 61: 0.029053025513399985\n",
      "Average loss epoch 62: 0.0291616711547929\n",
      "Average loss epoch 63: 0.027759192842189397\n",
      "Average loss epoch 64: 0.031060373223447634\n",
      "Average loss epoch 65: 0.0313721574825522\n",
      "Average loss epoch 66: 0.02871284431508072\n",
      "Average loss epoch 67: 0.025176183995137984\n",
      "Average loss epoch 68: 0.028146607570663514\n",
      "Average loss epoch 69: 0.02689854669271507\n",
      "Average loss epoch 70: 0.02794440417054379\n",
      "Average loss epoch 71: 0.029876097749535726\n",
      "Average loss epoch 72: 0.02853051698374971\n",
      "Average loss epoch 73: 0.029325638445111636\n",
      "Average loss epoch 74: 0.02960329006695859\n",
      "Average loss epoch 75: 0.028822314760116773\n",
      "Average loss epoch 76: 0.025258658266687228\n",
      "Average loss epoch 77: 0.02668373428598464\n",
      "Average loss epoch 78: 0.026475098275657846\n",
      "Average loss epoch 79: 0.02661098507624642\n",
      "Average loss epoch 80: 0.025801948041037144\n",
      "Average loss epoch 81: 0.02576902072667797\n",
      "Average loss epoch 82: 0.027401770047286403\n",
      "Average loss epoch 83: 0.02460990659022164\n",
      "Average loss epoch 84: 0.024741871282458305\n",
      "Average loss epoch 85: 0.023578409736575646\n",
      "Average loss epoch 86: 0.02662779643661647\n",
      "Average loss epoch 87: 0.026122471487375898\n",
      "Average loss epoch 88: 0.022890962951894953\n",
      "Average loss epoch 89: 0.022977924844789728\n",
      "Average loss epoch 90: 0.023637147298274198\n",
      "Average loss epoch 91: 0.024193007960288882\n",
      "Average loss epoch 92: 0.025025640448955732\n",
      "Average loss epoch 93: 0.020694739372408556\n",
      "Average loss epoch 94: 0.02350100166399345\n",
      "Average loss epoch 95: 0.022573264907592924\n",
      "Average loss epoch 96: 0.02436097565533541\n",
      "Average loss epoch 97: 0.024210940215249208\n",
      "Average loss epoch 98: 0.0245324861319196\n",
      "Average loss epoch 99: 0.02474820855395677\n",
      "Average loss epoch 100: 0.022430346325631734\n",
      "Average loss epoch 101: 0.024312713731811427\n",
      "Average loss epoch 102: 0.020232212647064544\n",
      "Average loss epoch 103: 0.021009079433072394\n",
      "Average loss epoch 104: 0.02132786339514445\n",
      "Average loss epoch 105: 0.022481117538985945\n",
      "Average loss epoch 106: 0.022933041820494094\n",
      "Average loss epoch 107: 0.023116774247767768\n",
      "Average loss epoch 108: 0.02259000566587827\n",
      "Average loss epoch 109: 0.021932362161019694\n",
      "Average loss epoch 110: 0.021570854385113605\n",
      "Average loss epoch 111: 0.020142085384577513\n",
      "Average loss epoch 112: 0.021124491576871303\n",
      "Average loss epoch 113: 0.021054720645335232\n",
      "Average loss epoch 114: 0.023900915903396137\n",
      "Average loss epoch 115: 0.022304371495461352\n",
      "Average loss epoch 116: 0.021998676240757527\n",
      "Average loss epoch 117: 0.020775278721213618\n",
      "Average loss epoch 118: 0.020976592364468586\n",
      "Average loss epoch 119: 0.019525023560239888\n",
      "Average loss epoch 120: 0.018490555110848815\n",
      "Average loss epoch 121: 0.021561791100887424\n",
      "Average loss epoch 122: 0.01949061787138893\n",
      "Average loss epoch 123: 0.018158674157424785\n",
      "Average loss epoch 124: 0.02062169076204648\n",
      "Average loss epoch 125: 0.020559213259126817\n",
      "Average loss epoch 126: 0.021544438852084295\n",
      "Average loss epoch 127: 0.02151033101890619\n",
      "Average loss epoch 128: 0.018829551568933737\n",
      "Average loss epoch 129: 0.020291360673955948\n",
      "Average loss epoch 130: 0.019517750126771004\n",
      "Average loss epoch 131: 0.01823015811104571\n",
      "Average loss epoch 132: 0.018546252507089734\n",
      "Average loss epoch 133: 0.01902138767328775\n",
      "Average loss epoch 134: 0.021172637840141062\n",
      "Average loss epoch 135: 0.020519563288506223\n",
      "Average loss epoch 136: 0.019887498841037816\n",
      "Average loss epoch 137: 0.018616104692168463\n",
      "Average loss epoch 138: 0.020184507245354564\n",
      "Average loss epoch 139: 0.02069550931941126\n",
      "Average loss epoch 140: 0.020164294951708517\n",
      "Average loss epoch 141: 0.01811217665063026\n",
      "Average loss epoch 142: 0.018760426426915642\n",
      "Average loss epoch 143: 0.017611762822092137\n",
      "Average loss epoch 144: 0.01899394005423882\n",
      "Average loss epoch 145: 0.01993416008326251\n",
      "Average loss epoch 146: 0.01816332195102124\n",
      "Average loss epoch 147: 0.018335448223314135\n",
      "Average loss epoch 148: 0.01844416480624578\n",
      "Average loss epoch 149: 0.018750648062119138\n",
      "Average loss epoch 150: 0.018303231458031685\n",
      "Average loss epoch 151: 0.018822645946954177\n",
      "Average loss epoch 152: 0.019281379462907387\n",
      "Average loss epoch 153: 0.019279119362793515\n",
      "Average loss epoch 154: 0.01879703579688114\n",
      "Average loss epoch 155: 0.017818707822621845\n",
      "Average loss epoch 156: 0.016972967760776666\n",
      "Average loss epoch 157: 0.017984234414576927\n",
      "Average loss epoch 158: 0.018431499458971285\n",
      "Average loss epoch 159: 0.016184980001857627\n",
      "Average loss epoch 160: 0.01777544523432642\n",
      "Average loss epoch 161: 0.01633712302057821\n",
      "Average loss epoch 162: 0.01607439585822138\n",
      "Average loss epoch 163: 0.017750729865472867\n",
      "Average loss epoch 164: 0.01801943301144883\n",
      "Average loss epoch 165: 0.016515128476606144\n",
      "Average loss epoch 166: 0.017085831433917596\n",
      "Average loss epoch 167: 0.017050323662974775\n",
      "Average loss epoch 168: 0.017076253262130876\n",
      "Average loss epoch 169: 0.01685538955650806\n",
      "Average loss epoch 170: 0.01762724991356867\n",
      "Average loss epoch 171: 0.015659110622793855\n",
      "Average loss epoch 172: 0.018961773092954236\n",
      "Average loss epoch 173: 0.014952432995261711\n",
      "Average loss epoch 174: 0.016109673271530144\n",
      "Average loss epoch 175: 0.016515124397717903\n",
      "Average loss epoch 176: 0.016565291950844715\n",
      "Average loss epoch 177: 0.018759886531820782\n",
      "Average loss epoch 178: 0.01747937644527198\n",
      "Average loss epoch 179: 0.015493932632712003\n",
      "Average loss epoch 180: 0.015554080628050125\n",
      "Average loss epoch 181: 0.0168765456726879\n",
      "Average loss epoch 182: 0.016692060777029703\n",
      "Average loss epoch 183: 0.017327489659938716\n",
      "Average loss epoch 184: 0.01702541018026733\n",
      "Average loss epoch 185: 0.01568646209910686\n",
      "Average loss epoch 186: 0.0173067284952501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 187: 0.01566367223423636\n",
      "Average loss epoch 188: 0.014671729533785136\n",
      "Average loss epoch 189: 0.01626886651311592\n",
      "Average loss epoch 190: 0.014927109596798214\n",
      "Average loss epoch 191: 0.014519936301762883\n",
      "Average loss epoch 192: 0.016163808913797\n",
      "Average loss epoch 193: 0.015991117465788515\n",
      "Average loss epoch 194: 0.017239719248489938\n",
      "Average loss epoch 195: 0.01667369432773476\n",
      "Average loss epoch 196: 0.01806180602131046\n",
      "Average loss epoch 197: 0.01715420129291156\n",
      "Average loss epoch 198: 0.01574820525091246\n",
      "Average loss epoch 199: 0.016504533573654348\n",
      "Total time: 301.74692010879517 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9578\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\t\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # TO-DO: run optimizer + fetch loss_batch\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X:X_batch, Y: Y_batch, keep_prob: 0.5})\n",
    "            \n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "    \n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        correct_preds_batch = sess.run(correct_preds, feed_dict={X: X_batch, Y:Y_batch, keep_prob:1})\n",
    "        #accuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y:Y_batch})  \n",
    "        #This way, accuracy_bath will be a list\n",
    "        \n",
    "        #print(type(accuracy_batch))\n",
    "        total_correct_preds += correct_preds_batch\n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full-layer neural network accuracy below 96%\n",
    "```Average loss epoch 199: 0.016504533573654348\n",
    "Total time: 301.74692010879517 seconds\n",
    "Optimization Finished!\n",
    "Accuracy 0.9578```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# Features are of the type float, and labels are of the type int\n",
    "X =  tf.placeholder(dtype= tf.float32, shape=(None, 784), name ='input_X')\n",
    "Y = tf.placeholder(dtype=tf.int16, shape=(None, 10), name = 'output_Y')\n",
    "isTrain = tf.placeholder(dtype=tf.bool, shape=[], name = 'Train_enable')\n",
    "\n",
    "imag = tf.reshape(X, shape=(-1, 28, 28, 1))\n",
    "conv1 = tf.layers.conv2d(\n",
    "      inputs=imag,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "conv3 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=128,\n",
    "      kernel_size=[5, 5],\n",
    "      kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Dense Layer\n",
    "pool3_flat = tf.reshape(pool3, [-1, 3 * 3 * 128])\n",
    "dense = tf.layers.dense(inputs=pool3_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training= isTrain)\n",
    "\n",
    "logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "Y_predicted = tf.argmax(logits, axis= 1)\n",
    "Y_true = tf.argmax(Y, axis= 1)\n",
    "correct_preds = tf.reduce_sum(tf.cast(tf.equal(Y_predicted, Y_true), tf.float32))\n",
    "\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(xentropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent to minimize loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= 0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.7313583569827481\n",
      "Average loss epoch 1: 0.1595021419784176\n",
      "Average loss epoch 2: 0.10928969036474406\n",
      "Average loss epoch 3: 0.08368257516733954\n",
      "Average loss epoch 4: 0.07370360853655317\n",
      "Average loss epoch 5: 0.0591595581271381\n",
      "Average loss epoch 6: 0.053877470040014974\n",
      "Average loss epoch 7: 0.048385685544774355\n",
      "Average loss epoch 8: 0.043193596502261185\n",
      "Average loss epoch 9: 0.03943725789366203\n",
      "Average loss epoch 10: 0.03516551532356121\n",
      "Average loss epoch 11: 0.0326293211465127\n",
      "Average loss epoch 12: 0.028863875278071546\n",
      "Average loss epoch 13: 0.026935691939814835\n",
      "Average loss epoch 14: 0.024451664268152318\n",
      "Average loss epoch 15: 0.02328879704533545\n",
      "Average loss epoch 16: 0.021098840532380495\n",
      "Average loss epoch 17: 0.019984008404904993\n",
      "Average loss epoch 18: 0.01819322693431489\n",
      "Average loss epoch 19: 0.01604412627301971\n",
      "Average loss epoch 20: 0.015333258368457867\n",
      "Average loss epoch 21: 0.01485249760264757\n",
      "Average loss epoch 22: 0.013210741714378558\n",
      "Average loss epoch 23: 0.012702885871017613\n",
      "Average loss epoch 24: 0.01130188914575118\n",
      "Average loss epoch 25: 0.011381701446055634\n",
      "Average loss epoch 26: 0.010687510691861683\n",
      "Average loss epoch 27: 0.009687922705371785\n",
      "Average loss epoch 28: 0.009238522936901182\n",
      "Average loss epoch 29: 0.009126445788522381\n",
      "Average loss epoch 30: 0.008262118827562483\n",
      "Average loss epoch 31: 0.0064588130260523945\n",
      "Average loss epoch 32: 0.007388946400067517\n",
      "Average loss epoch 33: 0.006051227923104034\n",
      "Average loss epoch 34: 0.005616422087089028\n",
      "Average loss epoch 35: 0.007565380495137352\n",
      "Average loss epoch 36: 0.004865096855329819\n",
      "Average loss epoch 37: 0.005320949592680897\n",
      "Average loss epoch 38: 0.004611230577942241\n",
      "Average loss epoch 39: 0.004292311723287996\n",
      "Average loss epoch 40: 0.003643511278254511\n",
      "Average loss epoch 41: 0.004355836192716596\n",
      "Average loss epoch 42: 0.004840804257557432\n",
      "Average loss epoch 43: 0.0036982387488856956\n",
      "Average loss epoch 44: 0.003534543171758104\n",
      "Average loss epoch 45: 0.003547772131607298\n",
      "Average loss epoch 46: 0.0032311409883740766\n",
      "Average loss epoch 47: 0.002590900324023912\n",
      "Average loss epoch 48: 0.005071571961766877\n",
      "Average loss epoch 49: 0.002424540932148273\n",
      "Average loss epoch 50: 0.0026022115393928534\n",
      "Average loss epoch 51: 0.0018110783890971727\n",
      "Average loss epoch 52: 0.0023461200952598995\n",
      "Average loss epoch 53: 0.0013370929758073152\n",
      "Average loss epoch 54: 0.0041366210871359235\n",
      "Average loss epoch 55: 0.0027036388465459023\n",
      "Average loss epoch 56: 0.0020365751467843183\n",
      "Average loss epoch 57: 0.0022716104576659822\n",
      "Average loss epoch 58: 0.0015818672520390039\n",
      "Average loss epoch 59: 0.00168759819582838\n",
      "Average loss epoch 60: 0.0016642239402712238\n",
      "Average loss epoch 61: 0.0025190286602575778\n",
      "Average loss epoch 62: 0.0019178433682338148\n",
      "Average loss epoch 63: 0.002799683327555902\n",
      "Average loss epoch 64: 0.001904226830260443\n",
      "Average loss epoch 65: 0.0009205623802580848\n",
      "Average loss epoch 66: 0.0015037048935916241\n",
      "Average loss epoch 67: 0.0014045121895017295\n",
      "Average loss epoch 68: 0.0015948909891687147\n",
      "Average loss epoch 69: 0.0017263816449298603\n",
      "Average loss epoch 70: 0.0022337759761990763\n",
      "Average loss epoch 71: 0.0019321794619759415\n",
      "Average loss epoch 72: 0.0011549947021215635\n",
      "Average loss epoch 73: 0.0015987848296624189\n",
      "Average loss epoch 74: 0.0010815093087883248\n",
      "Average loss epoch 75: 0.0008260163422079624\n",
      "Average loss epoch 76: 0.0013140004024424802\n",
      "Average loss epoch 77: 0.00217118365027031\n",
      "Average loss epoch 78: 0.0016956142236548121\n",
      "Average loss epoch 79: 0.0008136983625288883\n",
      "Average loss epoch 80: 0.0008224875767858398\n",
      "Average loss epoch 81: 0.0003482273171412769\n",
      "Average loss epoch 82: 0.00045738411973344166\n",
      "Average loss epoch 83: 0.00047169284160564727\n",
      "Average loss epoch 84: 0.0023605464738164503\n",
      "Average loss epoch 85: 0.0020559901130896645\n",
      "Average loss epoch 86: 0.0016252268832151085\n",
      "Average loss epoch 87: 0.0013377044628316208\n",
      "Average loss epoch 88: 0.0007362076516112261\n",
      "Average loss epoch 89: 0.0007838742155025815\n",
      "Average loss epoch 90: 0.0022114406316415336\n",
      "Average loss epoch 91: 0.0006664289546755176\n",
      "Average loss epoch 92: 0.0002583747235225461\n",
      "Average loss epoch 93: 0.0003062036104423573\n",
      "Average loss epoch 94: 0.0002790263261709765\n",
      "Average loss epoch 95: 0.002895649293660537\n",
      "Average loss epoch 96: 0.0010222867523080933\n",
      "Average loss epoch 97: 0.0005516704960166513\n",
      "Average loss epoch 98: 0.00024923107908787675\n",
      "Average loss epoch 99: 0.0004988794280995884\n",
      "Total time: 4403.048403263092 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9914\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\t\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # TO-DO: run optimizer + fetch loss_batch\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X:X_batch, Y: Y_batch, isTrain: True})\n",
    "            \n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "    \n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        correct_preds_batch = sess.run(correct_preds, feed_dict={X: X_batch, Y:Y_batch, isTrain:False})\n",
    "        #accuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y:Y_batch})  \n",
    "        #This way, accuracy_bath will be a list\n",
    "        \n",
    "        #print(type(accuracy_batch))\n",
    "        total_correct_preds += correct_preds_batch\n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bingo!!!\n",
    "```Average loss epoch 99: 0.0004988794280995884\n",
    "Total time: 4403.048403263092 seconds\n",
    "Optimization Finished!\n",
    "Accuracy 0.9914\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 3: Build a logistic regression model to predict whether someone has coronary heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_heart = pd.read_csv(r'./data/heart.csv')\n",
    "df_heart['famhist_Present'] = (df_heart['famhist'] == 'Present').astype(int)\n",
    "df_heart['famhist_Absent'] = (df_heart['famhist'] != 'Present').astype(int)\n",
    "df_heart = df_heart.drop('famhist', axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "      <th>famhist_Present</th>\n",
       "      <th>famhist_Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sbp  tobacco   ldl  adiposity  typea  obesity  alcohol  age  chd  \\\n",
       "0  160    12.00  5.73      23.11     49    25.30    97.20   52    1   \n",
       "1  144     0.01  4.41      28.61     55    28.87     2.06   63    1   \n",
       "2  118     0.08  3.48      32.28     52    29.14     3.81   46    0   \n",
       "3  170     7.50  6.41      38.03     51    31.99    24.26   58    1   \n",
       "4  134    13.60  3.50      27.78     60    25.99    57.34   49    1   \n",
       "\n",
       "   famhist_Present  famhist_Absent  \n",
       "0                1               0  \n",
       "1                0               1  \n",
       "2                1               0  \n",
       "3                1               0  \n",
       "4                1               0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg = df_heart[['sbp', 'tobacco', 'ldl','adiposity', 'typea', 'obesity', 'alcohol', 'age']].mean()\n",
    "std = df_heart[['sbp', 'tobacco', 'ldl','adiposity', 'typea', 'obesity', 'alcohol', 'age']].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw = df_heart[['sbp', 'tobacco', 'ldl','adiposity', 'typea', 'obesity', 'alcohol', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heart[['sbp', 'tobacco', 'ldl','adiposity', 'typea', 'obesity', 'alcohol', 'age']] =  (data_raw- avg) / std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "      <th>famhist_Present</th>\n",
       "      <th>famhist_Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.057417</td>\n",
       "      <td>1.821099</td>\n",
       "      <td>0.477894</td>\n",
       "      <td>-0.295183</td>\n",
       "      <td>-0.418017</td>\n",
       "      <td>-0.176594</td>\n",
       "      <td>3.274189</td>\n",
       "      <td>0.628654</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276789</td>\n",
       "      <td>-0.789382</td>\n",
       "      <td>-0.159507</td>\n",
       "      <td>0.411694</td>\n",
       "      <td>0.193134</td>\n",
       "      <td>0.670646</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>1.381617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.991731</td>\n",
       "      <td>-0.774141</td>\n",
       "      <td>-0.608585</td>\n",
       "      <td>0.883374</td>\n",
       "      <td>-0.112441</td>\n",
       "      <td>0.734723</td>\n",
       "      <td>-0.540597</td>\n",
       "      <td>0.217947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.545310</td>\n",
       "      <td>0.841352</td>\n",
       "      <td>0.806252</td>\n",
       "      <td>1.622382</td>\n",
       "      <td>-0.214300</td>\n",
       "      <td>1.411091</td>\n",
       "      <td>0.294742</td>\n",
       "      <td>1.039361</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.211103</td>\n",
       "      <td>2.169453</td>\n",
       "      <td>-0.598928</td>\n",
       "      <td>0.305020</td>\n",
       "      <td>0.702427</td>\n",
       "      <td>-0.012842</td>\n",
       "      <td>1.645991</td>\n",
       "      <td>0.423301</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sbp   tobacco       ldl  adiposity     typea   obesity   alcohol  \\\n",
       "0  1.057417  1.821099  0.477894  -0.295183 -0.418017 -0.176594  3.274189   \n",
       "1  0.276789 -0.789382 -0.159507   0.411694  0.193134  0.670646 -0.612081   \n",
       "2 -0.991731 -0.774141 -0.608585   0.883374 -0.112441  0.734723 -0.540597   \n",
       "3  1.545310  0.841352  0.806252   1.622382 -0.214300  1.411091  0.294742   \n",
       "4 -0.211103  2.169453 -0.598928   0.305020  0.702427 -0.012842  1.645991   \n",
       "\n",
       "        age  chd  famhist_Present  famhist_Absent  \n",
       "0  0.628654    1                1               0  \n",
       "1  1.381617    1                0               1  \n",
       "2  0.217947    0                1               0  \n",
       "3  1.039361    1                1               0  \n",
       "4  0.423301    1                1               0  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df_heart, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "      <th>famhist_Present</th>\n",
       "      <th>famhist_Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1.350153</td>\n",
       "      <td>-0.617382</td>\n",
       "      <td>0.429606</td>\n",
       "      <td>1.388470</td>\n",
       "      <td>-0.316158</td>\n",
       "      <td>2.058981</td>\n",
       "      <td>0.480192</td>\n",
       "      <td>1.176264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>-0.991731</td>\n",
       "      <td>-0.765432</td>\n",
       "      <td>-1.342562</td>\n",
       "      <td>-0.655048</td>\n",
       "      <td>-1.640320</td>\n",
       "      <td>-1.432029</td>\n",
       "      <td>-0.597376</td>\n",
       "      <td>-1.698685</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.601417</td>\n",
       "      <td>0.318821</td>\n",
       "      <td>-0.859683</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.193134</td>\n",
       "      <td>-0.124384</td>\n",
       "      <td>-0.192165</td>\n",
       "      <td>-0.329662</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.471946</td>\n",
       "      <td>-0.791559</td>\n",
       "      <td>-0.038787</td>\n",
       "      <td>-0.130674</td>\n",
       "      <td>-0.316158</td>\n",
       "      <td>-0.186087</td>\n",
       "      <td>-0.531611</td>\n",
       "      <td>-1.082625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.796574</td>\n",
       "      <td>0.079327</td>\n",
       "      <td>0.241283</td>\n",
       "      <td>0.319158</td>\n",
       "      <td>-0.825451</td>\n",
       "      <td>0.112939</td>\n",
       "      <td>-0.696228</td>\n",
       "      <td>1.244715</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sbp   tobacco       ldl  adiposity     typea   obesity   alcohol  \\\n",
       "411  1.350153 -0.617382  0.429606   1.388470 -0.316158  2.058981  0.480192   \n",
       "265 -0.991731 -0.765432 -1.342562  -0.655048 -1.640320 -1.432029 -0.597376   \n",
       "57  -0.601417  0.318821 -0.859683   0.140510  0.193134 -0.124384 -0.192165   \n",
       "199  0.471946 -0.791559 -0.038787  -0.130674 -0.316158 -0.186087 -0.531611   \n",
       "175 -0.796574  0.079327  0.241283   0.319158 -0.825451  0.112939 -0.696228   \n",
       "\n",
       "          age  chd  famhist_Present  famhist_Absent  \n",
       "411  1.176264    0                0               1  \n",
       "265 -1.698685    0                0               1  \n",
       "57  -0.329662    1                0               1  \n",
       "199 -1.082625    0                0               1  \n",
       "175  1.244715    1                1               0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_train = train_set.drop('chd', axis=1).copy().as_matrix()\n",
    "feature_test = test_set.drop('chd', axis=1).copy().as_matrix()\n",
    "labels_train = train_set['chd'].copy().as_matrix()\n",
    "labels_test = test_set['chd'].copy().as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(features,labels, batch_size):\n",
    "    num_batch = len(features) // batch_size\n",
    "    for i in range(num_batch):\n",
    "        yield features[i*batch_size: (i+1)*batch_size], labels[i*batch_size: (i+1)*batch_size]\n",
    "    yield features[num_batch * batch_size:], labels[num_batch * batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "features = tf.placeholder(dtype=tf.float32, shape=[None, 10], name = 'features')\n",
    "labels = tf.placeholder(dtype= tf.float32, shape=[None], name ='labels')\n",
    "isTraining = tf.placeholder(dtype=tf.bool, shape =[], name='Training_enable')\n",
    "\n",
    "batch_norm1= tf.layers.batch_normalization(features, training= isTraining)\n",
    "fc1 = tf.layers.dense(inputs=batch_norm1, units=64, activation=tf.nn.relu)\n",
    "\n",
    "batch_norm2= tf.layers.batch_normalization(fc1, training= isTraining)\n",
    "fc2 = tf.layers.dense(inputs=batch_norm2, units=32, activation=tf.nn.relu)\n",
    "\n",
    "batch_norm3= tf.layers.batch_normalization(fc2, training= isTraining)\n",
    "logits = tf.squeeze(tf.layers.dense(inputs=batch_norm3, units=1, activation=None))\n",
    "sigmoid = tf.sigmoid(logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "\n",
    "labels_predicted = tf.cast(tf.greater(sigmoid, 0.5), tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(labels_predicted, labels), tf.float32))\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ephoch:0,batch:1, accuracy:0.53125\n",
      "ephoch:0,batch:2, accuracy:0.53125\n",
      "ephoch:0,batch:3, accuracy:0.5\n",
      "ephoch:0,batch:4, accuracy:0.4375\n",
      "ephoch:0,batch:5, accuracy:0.5625\n",
      "ephoch:0,batch:6, accuracy:0.40625\n",
      "ephoch:0,batch:7, accuracy:0.46875\n",
      "ephoch:0,batch:8, accuracy:0.40625\n",
      "ephoch:0,batch:9, accuracy:0.46875\n",
      "ephoch:0,batch:10, accuracy:0.4375\n",
      "ephoch:0,batch:11, accuracy:0.34375\n",
      "ephoch:0,batch:12, accuracy:0.47058823704719543\n",
      "ephoch:1,batch:1, accuracy:0.5625\n",
      "ephoch:1,batch:2, accuracy:0.59375\n",
      "ephoch:1,batch:3, accuracy:0.5\n",
      "ephoch:1,batch:4, accuracy:0.4375\n",
      "ephoch:1,batch:5, accuracy:0.59375\n",
      "ephoch:1,batch:6, accuracy:0.4375\n",
      "ephoch:1,batch:7, accuracy:0.5\n",
      "ephoch:1,batch:8, accuracy:0.4375\n",
      "ephoch:1,batch:9, accuracy:0.4375\n",
      "ephoch:1,batch:10, accuracy:0.4375\n",
      "ephoch:1,batch:11, accuracy:0.34375\n",
      "ephoch:1,batch:12, accuracy:0.47058823704719543\n",
      "ephoch:2,batch:1, accuracy:0.5625\n",
      "ephoch:2,batch:2, accuracy:0.5625\n",
      "ephoch:2,batch:3, accuracy:0.4375\n",
      "ephoch:2,batch:4, accuracy:0.5\n",
      "ephoch:2,batch:5, accuracy:0.625\n",
      "ephoch:2,batch:6, accuracy:0.5\n",
      "ephoch:2,batch:7, accuracy:0.5\n",
      "ephoch:2,batch:8, accuracy:0.4375\n",
      "ephoch:2,batch:9, accuracy:0.4375\n",
      "ephoch:2,batch:10, accuracy:0.5\n",
      "ephoch:2,batch:11, accuracy:0.40625\n",
      "ephoch:2,batch:12, accuracy:0.529411792755127\n",
      "ephoch:3,batch:1, accuracy:0.59375\n",
      "ephoch:3,batch:2, accuracy:0.59375\n",
      "ephoch:3,batch:3, accuracy:0.46875\n",
      "ephoch:3,batch:4, accuracy:0.46875\n",
      "ephoch:3,batch:5, accuracy:0.59375\n",
      "ephoch:3,batch:6, accuracy:0.5\n",
      "ephoch:3,batch:7, accuracy:0.46875\n",
      "ephoch:3,batch:8, accuracy:0.4375\n",
      "ephoch:3,batch:9, accuracy:0.4375\n",
      "ephoch:3,batch:10, accuracy:0.5\n",
      "ephoch:3,batch:11, accuracy:0.40625\n",
      "ephoch:3,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:4,batch:1, accuracy:0.59375\n",
      "ephoch:4,batch:2, accuracy:0.5625\n",
      "ephoch:4,batch:3, accuracy:0.4375\n",
      "ephoch:4,batch:4, accuracy:0.46875\n",
      "ephoch:4,batch:5, accuracy:0.625\n",
      "ephoch:4,batch:6, accuracy:0.53125\n",
      "ephoch:4,batch:7, accuracy:0.46875\n",
      "ephoch:4,batch:8, accuracy:0.46875\n",
      "ephoch:4,batch:9, accuracy:0.46875\n",
      "ephoch:4,batch:10, accuracy:0.5\n",
      "ephoch:4,batch:11, accuracy:0.40625\n",
      "ephoch:4,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:5,batch:1, accuracy:0.5625\n",
      "ephoch:5,batch:2, accuracy:0.59375\n",
      "ephoch:5,batch:3, accuracy:0.5\n",
      "ephoch:5,batch:4, accuracy:0.46875\n",
      "ephoch:5,batch:5, accuracy:0.65625\n",
      "ephoch:5,batch:6, accuracy:0.53125\n",
      "ephoch:5,batch:7, accuracy:0.5\n",
      "ephoch:5,batch:8, accuracy:0.5\n",
      "ephoch:5,batch:9, accuracy:0.53125\n",
      "ephoch:5,batch:10, accuracy:0.5\n",
      "ephoch:5,batch:11, accuracy:0.4375\n",
      "ephoch:5,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:6,batch:1, accuracy:0.625\n",
      "ephoch:6,batch:2, accuracy:0.59375\n",
      "ephoch:6,batch:3, accuracy:0.5\n",
      "ephoch:6,batch:4, accuracy:0.5625\n",
      "ephoch:6,batch:5, accuracy:0.6875\n",
      "ephoch:6,batch:6, accuracy:0.53125\n",
      "ephoch:6,batch:7, accuracy:0.5\n",
      "ephoch:6,batch:8, accuracy:0.4375\n",
      "ephoch:6,batch:9, accuracy:0.5\n",
      "ephoch:6,batch:10, accuracy:0.53125\n",
      "ephoch:6,batch:11, accuracy:0.46875\n",
      "ephoch:6,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:7,batch:1, accuracy:0.5625\n",
      "ephoch:7,batch:2, accuracy:0.5625\n",
      "ephoch:7,batch:3, accuracy:0.53125\n",
      "ephoch:7,batch:4, accuracy:0.625\n",
      "ephoch:7,batch:5, accuracy:0.6875\n",
      "ephoch:7,batch:6, accuracy:0.5\n",
      "ephoch:7,batch:7, accuracy:0.53125\n",
      "ephoch:7,batch:8, accuracy:0.46875\n",
      "ephoch:7,batch:9, accuracy:0.53125\n",
      "ephoch:7,batch:10, accuracy:0.625\n",
      "ephoch:7,batch:11, accuracy:0.5\n",
      "ephoch:7,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:8,batch:1, accuracy:0.5625\n",
      "ephoch:8,batch:2, accuracy:0.5625\n",
      "ephoch:8,batch:3, accuracy:0.5625\n",
      "ephoch:8,batch:4, accuracy:0.6875\n",
      "ephoch:8,batch:5, accuracy:0.71875\n",
      "ephoch:8,batch:6, accuracy:0.53125\n",
      "ephoch:8,batch:7, accuracy:0.5\n",
      "ephoch:8,batch:8, accuracy:0.4375\n",
      "ephoch:8,batch:9, accuracy:0.53125\n",
      "ephoch:8,batch:10, accuracy:0.625\n",
      "ephoch:8,batch:11, accuracy:0.5\n",
      "ephoch:8,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:9,batch:1, accuracy:0.5625\n",
      "ephoch:9,batch:2, accuracy:0.5625\n",
      "ephoch:9,batch:3, accuracy:0.5625\n",
      "ephoch:9,batch:4, accuracy:0.6875\n",
      "ephoch:9,batch:5, accuracy:0.6875\n",
      "ephoch:9,batch:6, accuracy:0.5625\n",
      "ephoch:9,batch:7, accuracy:0.53125\n",
      "ephoch:9,batch:8, accuracy:0.4375\n",
      "ephoch:9,batch:9, accuracy:0.5\n",
      "ephoch:9,batch:10, accuracy:0.625\n",
      "ephoch:9,batch:11, accuracy:0.5625\n",
      "ephoch:9,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:10,batch:1, accuracy:0.59375\n",
      "ephoch:10,batch:2, accuracy:0.5625\n",
      "ephoch:10,batch:3, accuracy:0.59375\n",
      "ephoch:10,batch:4, accuracy:0.6875\n",
      "ephoch:10,batch:5, accuracy:0.6875\n",
      "ephoch:10,batch:6, accuracy:0.5625\n",
      "ephoch:10,batch:7, accuracy:0.53125\n",
      "ephoch:10,batch:8, accuracy:0.4375\n",
      "ephoch:10,batch:9, accuracy:0.5\n",
      "ephoch:10,batch:10, accuracy:0.65625\n",
      "ephoch:10,batch:11, accuracy:0.5625\n",
      "ephoch:10,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:11,batch:1, accuracy:0.65625\n",
      "ephoch:11,batch:2, accuracy:0.59375\n",
      "ephoch:11,batch:3, accuracy:0.625\n",
      "ephoch:11,batch:4, accuracy:0.71875\n",
      "ephoch:11,batch:5, accuracy:0.6875\n",
      "ephoch:11,batch:6, accuracy:0.59375\n",
      "ephoch:11,batch:7, accuracy:0.53125\n",
      "ephoch:11,batch:8, accuracy:0.4375\n",
      "ephoch:11,batch:9, accuracy:0.5\n",
      "ephoch:11,batch:10, accuracy:0.65625\n",
      "ephoch:11,batch:11, accuracy:0.5625\n",
      "ephoch:11,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:12,batch:1, accuracy:0.65625\n",
      "ephoch:12,batch:2, accuracy:0.59375\n",
      "ephoch:12,batch:3, accuracy:0.65625\n",
      "ephoch:12,batch:4, accuracy:0.71875\n",
      "ephoch:12,batch:5, accuracy:0.65625\n",
      "ephoch:12,batch:6, accuracy:0.65625\n",
      "ephoch:12,batch:7, accuracy:0.5625\n",
      "ephoch:12,batch:8, accuracy:0.46875\n",
      "ephoch:12,batch:9, accuracy:0.5\n",
      "ephoch:12,batch:10, accuracy:0.65625\n",
      "ephoch:12,batch:11, accuracy:0.5625\n",
      "ephoch:12,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:13,batch:1, accuracy:0.65625\n",
      "ephoch:13,batch:2, accuracy:0.59375\n",
      "ephoch:13,batch:3, accuracy:0.65625\n",
      "ephoch:13,batch:4, accuracy:0.71875\n",
      "ephoch:13,batch:5, accuracy:0.65625\n",
      "ephoch:13,batch:6, accuracy:0.71875\n",
      "ephoch:13,batch:7, accuracy:0.59375\n",
      "ephoch:13,batch:8, accuracy:0.5\n",
      "ephoch:13,batch:9, accuracy:0.53125\n",
      "ephoch:13,batch:10, accuracy:0.65625\n",
      "ephoch:13,batch:11, accuracy:0.625\n",
      "ephoch:13,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:14,batch:1, accuracy:0.625\n",
      "ephoch:14,batch:2, accuracy:0.59375\n",
      "ephoch:14,batch:3, accuracy:0.65625\n",
      "ephoch:14,batch:4, accuracy:0.71875\n",
      "ephoch:14,batch:5, accuracy:0.65625\n",
      "ephoch:14,batch:6, accuracy:0.71875\n",
      "ephoch:14,batch:7, accuracy:0.59375\n",
      "ephoch:14,batch:8, accuracy:0.5\n",
      "ephoch:14,batch:9, accuracy:0.53125\n",
      "ephoch:14,batch:10, accuracy:0.65625\n",
      "ephoch:14,batch:11, accuracy:0.625\n",
      "ephoch:14,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:15,batch:1, accuracy:0.65625\n",
      "ephoch:15,batch:2, accuracy:0.625\n",
      "ephoch:15,batch:3, accuracy:0.65625\n",
      "ephoch:15,batch:4, accuracy:0.71875\n",
      "ephoch:15,batch:5, accuracy:0.65625\n",
      "ephoch:15,batch:6, accuracy:0.71875\n",
      "ephoch:15,batch:7, accuracy:0.59375\n",
      "ephoch:15,batch:8, accuracy:0.5\n",
      "ephoch:15,batch:9, accuracy:0.5625\n",
      "ephoch:15,batch:10, accuracy:0.65625\n",
      "ephoch:15,batch:11, accuracy:0.625\n",
      "ephoch:15,batch:12, accuracy:0.5882353186607361\n",
      "ephoch:16,batch:1, accuracy:0.65625\n",
      "ephoch:16,batch:2, accuracy:0.625\n",
      "ephoch:16,batch:3, accuracy:0.65625\n",
      "ephoch:16,batch:4, accuracy:0.71875\n",
      "ephoch:16,batch:5, accuracy:0.65625\n",
      "ephoch:16,batch:6, accuracy:0.71875\n",
      "ephoch:16,batch:7, accuracy:0.59375\n",
      "ephoch:16,batch:8, accuracy:0.5\n",
      "ephoch:16,batch:9, accuracy:0.59375\n",
      "ephoch:16,batch:10, accuracy:0.65625\n",
      "ephoch:16,batch:11, accuracy:0.65625\n",
      "ephoch:16,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:17,batch:1, accuracy:0.65625\n",
      "ephoch:17,batch:2, accuracy:0.625\n",
      "ephoch:17,batch:3, accuracy:0.65625\n",
      "ephoch:17,batch:4, accuracy:0.71875\n",
      "ephoch:17,batch:5, accuracy:0.6875\n",
      "ephoch:17,batch:6, accuracy:0.71875\n",
      "ephoch:17,batch:7, accuracy:0.59375\n",
      "ephoch:17,batch:8, accuracy:0.53125\n",
      "ephoch:17,batch:9, accuracy:0.625\n",
      "ephoch:17,batch:10, accuracy:0.65625\n",
      "ephoch:17,batch:11, accuracy:0.65625\n",
      "ephoch:17,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:18,batch:1, accuracy:0.6875\n",
      "ephoch:18,batch:2, accuracy:0.59375\n",
      "ephoch:18,batch:3, accuracy:0.65625\n",
      "ephoch:18,batch:4, accuracy:0.75\n",
      "ephoch:18,batch:5, accuracy:0.65625\n",
      "ephoch:18,batch:6, accuracy:0.6875\n",
      "ephoch:18,batch:7, accuracy:0.59375\n",
      "ephoch:18,batch:8, accuracy:0.53125\n",
      "ephoch:18,batch:9, accuracy:0.65625\n",
      "ephoch:18,batch:10, accuracy:0.65625\n",
      "ephoch:18,batch:11, accuracy:0.65625\n",
      "ephoch:18,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:19,batch:1, accuracy:0.71875\n",
      "ephoch:19,batch:2, accuracy:0.59375\n",
      "ephoch:19,batch:3, accuracy:0.65625\n",
      "ephoch:19,batch:4, accuracy:0.75\n",
      "ephoch:19,batch:5, accuracy:0.65625\n",
      "ephoch:19,batch:6, accuracy:0.6875\n",
      "ephoch:19,batch:7, accuracy:0.59375\n",
      "ephoch:19,batch:8, accuracy:0.59375\n",
      "ephoch:19,batch:9, accuracy:0.65625\n",
      "ephoch:19,batch:10, accuracy:0.65625\n",
      "ephoch:19,batch:11, accuracy:0.65625\n",
      "ephoch:19,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:20,batch:1, accuracy:0.71875\n",
      "ephoch:20,batch:2, accuracy:0.625\n",
      "ephoch:20,batch:3, accuracy:0.65625\n",
      "ephoch:20,batch:4, accuracy:0.75\n",
      "ephoch:20,batch:5, accuracy:0.65625\n",
      "ephoch:20,batch:6, accuracy:0.6875\n",
      "ephoch:20,batch:7, accuracy:0.59375\n",
      "ephoch:20,batch:8, accuracy:0.59375\n",
      "ephoch:20,batch:9, accuracy:0.65625\n",
      "ephoch:20,batch:10, accuracy:0.65625\n",
      "ephoch:20,batch:11, accuracy:0.65625\n",
      "ephoch:20,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:21,batch:1, accuracy:0.71875\n",
      "ephoch:21,batch:2, accuracy:0.625\n",
      "ephoch:21,batch:3, accuracy:0.6875\n",
      "ephoch:21,batch:4, accuracy:0.75\n",
      "ephoch:21,batch:5, accuracy:0.6875\n",
      "ephoch:21,batch:6, accuracy:0.6875\n",
      "ephoch:21,batch:7, accuracy:0.59375\n",
      "ephoch:21,batch:8, accuracy:0.59375\n",
      "ephoch:21,batch:9, accuracy:0.6875\n",
      "ephoch:21,batch:10, accuracy:0.65625\n",
      "ephoch:21,batch:11, accuracy:0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ephoch:21,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:22,batch:1, accuracy:0.75\n",
      "ephoch:22,batch:2, accuracy:0.625\n",
      "ephoch:22,batch:3, accuracy:0.6875\n",
      "ephoch:22,batch:4, accuracy:0.78125\n",
      "ephoch:22,batch:5, accuracy:0.6875\n",
      "ephoch:22,batch:6, accuracy:0.6875\n",
      "ephoch:22,batch:7, accuracy:0.59375\n",
      "ephoch:22,batch:8, accuracy:0.625\n",
      "ephoch:22,batch:9, accuracy:0.6875\n",
      "ephoch:22,batch:10, accuracy:0.625\n",
      "ephoch:22,batch:11, accuracy:0.65625\n",
      "ephoch:22,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:23,batch:1, accuracy:0.75\n",
      "ephoch:23,batch:2, accuracy:0.625\n",
      "ephoch:23,batch:3, accuracy:0.6875\n",
      "ephoch:23,batch:4, accuracy:0.78125\n",
      "ephoch:23,batch:5, accuracy:0.6875\n",
      "ephoch:23,batch:6, accuracy:0.6875\n",
      "ephoch:23,batch:7, accuracy:0.625\n",
      "ephoch:23,batch:8, accuracy:0.6875\n",
      "ephoch:23,batch:9, accuracy:0.71875\n",
      "ephoch:23,batch:10, accuracy:0.65625\n",
      "ephoch:23,batch:11, accuracy:0.65625\n",
      "ephoch:23,batch:12, accuracy:0.6470588445663452\n",
      "ephoch:24,batch:1, accuracy:0.75\n",
      "ephoch:24,batch:2, accuracy:0.625\n",
      "ephoch:24,batch:3, accuracy:0.6875\n",
      "ephoch:24,batch:4, accuracy:0.78125\n",
      "ephoch:24,batch:5, accuracy:0.6875\n",
      "ephoch:24,batch:6, accuracy:0.6875\n",
      "ephoch:24,batch:7, accuracy:0.625\n",
      "ephoch:24,batch:8, accuracy:0.6875\n",
      "ephoch:24,batch:9, accuracy:0.6875\n",
      "ephoch:24,batch:10, accuracy:0.65625\n",
      "ephoch:24,batch:11, accuracy:0.65625\n",
      "ephoch:24,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:25,batch:1, accuracy:0.75\n",
      "ephoch:25,batch:2, accuracy:0.65625\n",
      "ephoch:25,batch:3, accuracy:0.6875\n",
      "ephoch:25,batch:4, accuracy:0.78125\n",
      "ephoch:25,batch:5, accuracy:0.6875\n",
      "ephoch:25,batch:6, accuracy:0.6875\n",
      "ephoch:25,batch:7, accuracy:0.65625\n",
      "ephoch:25,batch:8, accuracy:0.75\n",
      "ephoch:25,batch:9, accuracy:0.6875\n",
      "ephoch:25,batch:10, accuracy:0.6875\n",
      "ephoch:25,batch:11, accuracy:0.65625\n",
      "ephoch:25,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:26,batch:1, accuracy:0.75\n",
      "ephoch:26,batch:2, accuracy:0.65625\n",
      "ephoch:26,batch:3, accuracy:0.6875\n",
      "ephoch:26,batch:4, accuracy:0.78125\n",
      "ephoch:26,batch:5, accuracy:0.6875\n",
      "ephoch:26,batch:6, accuracy:0.6875\n",
      "ephoch:26,batch:7, accuracy:0.65625\n",
      "ephoch:26,batch:8, accuracy:0.75\n",
      "ephoch:26,batch:9, accuracy:0.6875\n",
      "ephoch:26,batch:10, accuracy:0.71875\n",
      "ephoch:26,batch:11, accuracy:0.65625\n",
      "ephoch:26,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:27,batch:1, accuracy:0.75\n",
      "ephoch:27,batch:2, accuracy:0.65625\n",
      "ephoch:27,batch:3, accuracy:0.6875\n",
      "ephoch:27,batch:4, accuracy:0.8125\n",
      "ephoch:27,batch:5, accuracy:0.6875\n",
      "ephoch:27,batch:6, accuracy:0.6875\n",
      "ephoch:27,batch:7, accuracy:0.65625\n",
      "ephoch:27,batch:8, accuracy:0.75\n",
      "ephoch:27,batch:9, accuracy:0.6875\n",
      "ephoch:27,batch:10, accuracy:0.75\n",
      "ephoch:27,batch:11, accuracy:0.65625\n",
      "ephoch:27,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:28,batch:1, accuracy:0.75\n",
      "ephoch:28,batch:2, accuracy:0.65625\n",
      "ephoch:28,batch:3, accuracy:0.6875\n",
      "ephoch:28,batch:4, accuracy:0.8125\n",
      "ephoch:28,batch:5, accuracy:0.6875\n",
      "ephoch:28,batch:6, accuracy:0.6875\n",
      "ephoch:28,batch:7, accuracy:0.65625\n",
      "ephoch:28,batch:8, accuracy:0.75\n",
      "ephoch:28,batch:9, accuracy:0.6875\n",
      "ephoch:28,batch:10, accuracy:0.75\n",
      "ephoch:28,batch:11, accuracy:0.65625\n",
      "ephoch:28,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:29,batch:1, accuracy:0.75\n",
      "ephoch:29,batch:2, accuracy:0.65625\n",
      "ephoch:29,batch:3, accuracy:0.71875\n",
      "ephoch:29,batch:4, accuracy:0.8125\n",
      "ephoch:29,batch:5, accuracy:0.6875\n",
      "ephoch:29,batch:6, accuracy:0.6875\n",
      "ephoch:29,batch:7, accuracy:0.65625\n",
      "ephoch:29,batch:8, accuracy:0.75\n",
      "ephoch:29,batch:9, accuracy:0.6875\n",
      "ephoch:29,batch:10, accuracy:0.75\n",
      "ephoch:29,batch:11, accuracy:0.65625\n",
      "ephoch:29,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:30,batch:1, accuracy:0.75\n",
      "ephoch:30,batch:2, accuracy:0.65625\n",
      "ephoch:30,batch:3, accuracy:0.75\n",
      "ephoch:30,batch:4, accuracy:0.8125\n",
      "ephoch:30,batch:5, accuracy:0.71875\n",
      "ephoch:30,batch:6, accuracy:0.71875\n",
      "ephoch:30,batch:7, accuracy:0.6875\n",
      "ephoch:30,batch:8, accuracy:0.75\n",
      "ephoch:30,batch:9, accuracy:0.6875\n",
      "ephoch:30,batch:10, accuracy:0.75\n",
      "ephoch:30,batch:11, accuracy:0.6875\n",
      "ephoch:30,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:31,batch:1, accuracy:0.75\n",
      "ephoch:31,batch:2, accuracy:0.71875\n",
      "ephoch:31,batch:3, accuracy:0.75\n",
      "ephoch:31,batch:4, accuracy:0.8125\n",
      "ephoch:31,batch:5, accuracy:0.71875\n",
      "ephoch:31,batch:6, accuracy:0.71875\n",
      "ephoch:31,batch:7, accuracy:0.71875\n",
      "ephoch:31,batch:8, accuracy:0.75\n",
      "ephoch:31,batch:9, accuracy:0.6875\n",
      "ephoch:31,batch:10, accuracy:0.75\n",
      "ephoch:31,batch:11, accuracy:0.6875\n",
      "ephoch:31,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:32,batch:1, accuracy:0.75\n",
      "ephoch:32,batch:2, accuracy:0.71875\n",
      "ephoch:32,batch:3, accuracy:0.75\n",
      "ephoch:32,batch:4, accuracy:0.8125\n",
      "ephoch:32,batch:5, accuracy:0.71875\n",
      "ephoch:32,batch:6, accuracy:0.71875\n",
      "ephoch:32,batch:7, accuracy:0.71875\n",
      "ephoch:32,batch:8, accuracy:0.75\n",
      "ephoch:32,batch:9, accuracy:0.6875\n",
      "ephoch:32,batch:10, accuracy:0.75\n",
      "ephoch:32,batch:11, accuracy:0.6875\n",
      "ephoch:32,batch:12, accuracy:0.7058823704719543\n",
      "ephoch:33,batch:1, accuracy:0.75\n",
      "ephoch:33,batch:2, accuracy:0.71875\n",
      "ephoch:33,batch:3, accuracy:0.75\n",
      "ephoch:33,batch:4, accuracy:0.8125\n",
      "ephoch:33,batch:5, accuracy:0.75\n",
      "ephoch:33,batch:6, accuracy:0.71875\n",
      "ephoch:33,batch:7, accuracy:0.71875\n",
      "ephoch:33,batch:8, accuracy:0.75\n",
      "ephoch:33,batch:9, accuracy:0.71875\n",
      "ephoch:33,batch:10, accuracy:0.75\n",
      "ephoch:33,batch:11, accuracy:0.6875\n",
      "ephoch:33,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:34,batch:1, accuracy:0.75\n",
      "ephoch:34,batch:2, accuracy:0.71875\n",
      "ephoch:34,batch:3, accuracy:0.75\n",
      "ephoch:34,batch:4, accuracy:0.8125\n",
      "ephoch:34,batch:5, accuracy:0.75\n",
      "ephoch:34,batch:6, accuracy:0.71875\n",
      "ephoch:34,batch:7, accuracy:0.71875\n",
      "ephoch:34,batch:8, accuracy:0.78125\n",
      "ephoch:34,batch:9, accuracy:0.71875\n",
      "ephoch:34,batch:10, accuracy:0.75\n",
      "ephoch:34,batch:11, accuracy:0.75\n",
      "ephoch:34,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:35,batch:1, accuracy:0.75\n",
      "ephoch:35,batch:2, accuracy:0.71875\n",
      "ephoch:35,batch:3, accuracy:0.75\n",
      "ephoch:35,batch:4, accuracy:0.84375\n",
      "ephoch:35,batch:5, accuracy:0.75\n",
      "ephoch:35,batch:6, accuracy:0.71875\n",
      "ephoch:35,batch:7, accuracy:0.71875\n",
      "ephoch:35,batch:8, accuracy:0.78125\n",
      "ephoch:35,batch:9, accuracy:0.6875\n",
      "ephoch:35,batch:10, accuracy:0.75\n",
      "ephoch:35,batch:11, accuracy:0.75\n",
      "ephoch:35,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:36,batch:1, accuracy:0.75\n",
      "ephoch:36,batch:2, accuracy:0.71875\n",
      "ephoch:36,batch:3, accuracy:0.75\n",
      "ephoch:36,batch:4, accuracy:0.84375\n",
      "ephoch:36,batch:5, accuracy:0.75\n",
      "ephoch:36,batch:6, accuracy:0.71875\n",
      "ephoch:36,batch:7, accuracy:0.71875\n",
      "ephoch:36,batch:8, accuracy:0.78125\n",
      "ephoch:36,batch:9, accuracy:0.6875\n",
      "ephoch:36,batch:10, accuracy:0.75\n",
      "ephoch:36,batch:11, accuracy:0.75\n",
      "ephoch:36,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:37,batch:1, accuracy:0.75\n",
      "ephoch:37,batch:2, accuracy:0.71875\n",
      "ephoch:37,batch:3, accuracy:0.75\n",
      "ephoch:37,batch:4, accuracy:0.84375\n",
      "ephoch:37,batch:5, accuracy:0.75\n",
      "ephoch:37,batch:6, accuracy:0.71875\n",
      "ephoch:37,batch:7, accuracy:0.71875\n",
      "ephoch:37,batch:8, accuracy:0.78125\n",
      "ephoch:37,batch:9, accuracy:0.6875\n",
      "ephoch:37,batch:10, accuracy:0.75\n",
      "ephoch:37,batch:11, accuracy:0.75\n",
      "ephoch:37,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:38,batch:1, accuracy:0.75\n",
      "ephoch:38,batch:2, accuracy:0.71875\n",
      "ephoch:38,batch:3, accuracy:0.75\n",
      "ephoch:38,batch:4, accuracy:0.84375\n",
      "ephoch:38,batch:5, accuracy:0.75\n",
      "ephoch:38,batch:6, accuracy:0.71875\n",
      "ephoch:38,batch:7, accuracy:0.71875\n",
      "ephoch:38,batch:8, accuracy:0.78125\n",
      "ephoch:38,batch:9, accuracy:0.6875\n",
      "ephoch:38,batch:10, accuracy:0.78125\n",
      "ephoch:38,batch:11, accuracy:0.78125\n",
      "ephoch:38,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:39,batch:1, accuracy:0.75\n",
      "ephoch:39,batch:2, accuracy:0.75\n",
      "ephoch:39,batch:3, accuracy:0.75\n",
      "ephoch:39,batch:4, accuracy:0.84375\n",
      "ephoch:39,batch:5, accuracy:0.75\n",
      "ephoch:39,batch:6, accuracy:0.71875\n",
      "ephoch:39,batch:7, accuracy:0.71875\n",
      "ephoch:39,batch:8, accuracy:0.78125\n",
      "ephoch:39,batch:9, accuracy:0.71875\n",
      "ephoch:39,batch:10, accuracy:0.78125\n",
      "ephoch:39,batch:11, accuracy:0.78125\n",
      "ephoch:39,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:40,batch:1, accuracy:0.75\n",
      "ephoch:40,batch:2, accuracy:0.75\n",
      "ephoch:40,batch:3, accuracy:0.75\n",
      "ephoch:40,batch:4, accuracy:0.84375\n",
      "ephoch:40,batch:5, accuracy:0.75\n",
      "ephoch:40,batch:6, accuracy:0.75\n",
      "ephoch:40,batch:7, accuracy:0.71875\n",
      "ephoch:40,batch:8, accuracy:0.78125\n",
      "ephoch:40,batch:9, accuracy:0.71875\n",
      "ephoch:40,batch:10, accuracy:0.75\n",
      "ephoch:40,batch:11, accuracy:0.78125\n",
      "ephoch:40,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:41,batch:1, accuracy:0.75\n",
      "ephoch:41,batch:2, accuracy:0.75\n",
      "ephoch:41,batch:3, accuracy:0.75\n",
      "ephoch:41,batch:4, accuracy:0.8125\n",
      "ephoch:41,batch:5, accuracy:0.75\n",
      "ephoch:41,batch:6, accuracy:0.75\n",
      "ephoch:41,batch:7, accuracy:0.71875\n",
      "ephoch:41,batch:8, accuracy:0.78125\n",
      "ephoch:41,batch:9, accuracy:0.71875\n",
      "ephoch:41,batch:10, accuracy:0.75\n",
      "ephoch:41,batch:11, accuracy:0.78125\n",
      "ephoch:41,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:42,batch:1, accuracy:0.75\n",
      "ephoch:42,batch:2, accuracy:0.75\n",
      "ephoch:42,batch:3, accuracy:0.75\n",
      "ephoch:42,batch:4, accuracy:0.8125\n",
      "ephoch:42,batch:5, accuracy:0.75\n",
      "ephoch:42,batch:6, accuracy:0.75\n",
      "ephoch:42,batch:7, accuracy:0.6875\n",
      "ephoch:42,batch:8, accuracy:0.78125\n",
      "ephoch:42,batch:9, accuracy:0.71875\n",
      "ephoch:42,batch:10, accuracy:0.75\n",
      "ephoch:42,batch:11, accuracy:0.78125\n",
      "ephoch:42,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:43,batch:1, accuracy:0.78125\n",
      "ephoch:43,batch:2, accuracy:0.75\n",
      "ephoch:43,batch:3, accuracy:0.75\n",
      "ephoch:43,batch:4, accuracy:0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ephoch:43,batch:5, accuracy:0.75\n",
      "ephoch:43,batch:6, accuracy:0.75\n",
      "ephoch:43,batch:7, accuracy:0.6875\n",
      "ephoch:43,batch:8, accuracy:0.78125\n",
      "ephoch:43,batch:9, accuracy:0.71875\n",
      "ephoch:43,batch:10, accuracy:0.75\n",
      "ephoch:43,batch:11, accuracy:0.78125\n",
      "ephoch:43,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:44,batch:1, accuracy:0.78125\n",
      "ephoch:44,batch:2, accuracy:0.75\n",
      "ephoch:44,batch:3, accuracy:0.75\n",
      "ephoch:44,batch:4, accuracy:0.8125\n",
      "ephoch:44,batch:5, accuracy:0.75\n",
      "ephoch:44,batch:6, accuracy:0.75\n",
      "ephoch:44,batch:7, accuracy:0.71875\n",
      "ephoch:44,batch:8, accuracy:0.78125\n",
      "ephoch:44,batch:9, accuracy:0.71875\n",
      "ephoch:44,batch:10, accuracy:0.75\n",
      "ephoch:44,batch:11, accuracy:0.78125\n",
      "ephoch:44,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:45,batch:1, accuracy:0.78125\n",
      "ephoch:45,batch:2, accuracy:0.75\n",
      "ephoch:45,batch:3, accuracy:0.75\n",
      "ephoch:45,batch:4, accuracy:0.8125\n",
      "ephoch:45,batch:5, accuracy:0.75\n",
      "ephoch:45,batch:6, accuracy:0.75\n",
      "ephoch:45,batch:7, accuracy:0.75\n",
      "ephoch:45,batch:8, accuracy:0.78125\n",
      "ephoch:45,batch:9, accuracy:0.71875\n",
      "ephoch:45,batch:10, accuracy:0.75\n",
      "ephoch:45,batch:11, accuracy:0.78125\n",
      "ephoch:45,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:46,batch:1, accuracy:0.78125\n",
      "ephoch:46,batch:2, accuracy:0.75\n",
      "ephoch:46,batch:3, accuracy:0.75\n",
      "ephoch:46,batch:4, accuracy:0.84375\n",
      "ephoch:46,batch:5, accuracy:0.75\n",
      "ephoch:46,batch:6, accuracy:0.75\n",
      "ephoch:46,batch:7, accuracy:0.75\n",
      "ephoch:46,batch:8, accuracy:0.78125\n",
      "ephoch:46,batch:9, accuracy:0.71875\n",
      "ephoch:46,batch:10, accuracy:0.75\n",
      "ephoch:46,batch:11, accuracy:0.78125\n",
      "ephoch:46,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:47,batch:1, accuracy:0.78125\n",
      "ephoch:47,batch:2, accuracy:0.78125\n",
      "ephoch:47,batch:3, accuracy:0.75\n",
      "ephoch:47,batch:4, accuracy:0.875\n",
      "ephoch:47,batch:5, accuracy:0.75\n",
      "ephoch:47,batch:6, accuracy:0.75\n",
      "ephoch:47,batch:7, accuracy:0.75\n",
      "ephoch:47,batch:8, accuracy:0.75\n",
      "ephoch:47,batch:9, accuracy:0.71875\n",
      "ephoch:47,batch:10, accuracy:0.75\n",
      "ephoch:47,batch:11, accuracy:0.78125\n",
      "ephoch:47,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:48,batch:1, accuracy:0.78125\n",
      "ephoch:48,batch:2, accuracy:0.8125\n",
      "ephoch:48,batch:3, accuracy:0.75\n",
      "ephoch:48,batch:4, accuracy:0.875\n",
      "ephoch:48,batch:5, accuracy:0.75\n",
      "ephoch:48,batch:6, accuracy:0.75\n",
      "ephoch:48,batch:7, accuracy:0.75\n",
      "ephoch:48,batch:8, accuracy:0.75\n",
      "ephoch:48,batch:9, accuracy:0.71875\n",
      "ephoch:48,batch:10, accuracy:0.75\n",
      "ephoch:48,batch:11, accuracy:0.8125\n",
      "ephoch:48,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:49,batch:1, accuracy:0.8125\n",
      "ephoch:49,batch:2, accuracy:0.8125\n",
      "ephoch:49,batch:3, accuracy:0.75\n",
      "ephoch:49,batch:4, accuracy:0.875\n",
      "ephoch:49,batch:5, accuracy:0.75\n",
      "ephoch:49,batch:6, accuracy:0.75\n",
      "ephoch:49,batch:7, accuracy:0.75\n",
      "ephoch:49,batch:8, accuracy:0.75\n",
      "ephoch:49,batch:9, accuracy:0.75\n",
      "ephoch:49,batch:10, accuracy:0.75\n",
      "ephoch:49,batch:11, accuracy:0.8125\n",
      "ephoch:49,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:50,batch:1, accuracy:0.8125\n",
      "ephoch:50,batch:2, accuracy:0.8125\n",
      "ephoch:50,batch:3, accuracy:0.75\n",
      "ephoch:50,batch:4, accuracy:0.90625\n",
      "ephoch:50,batch:5, accuracy:0.75\n",
      "ephoch:50,batch:6, accuracy:0.75\n",
      "ephoch:50,batch:7, accuracy:0.75\n",
      "ephoch:50,batch:8, accuracy:0.75\n",
      "ephoch:50,batch:9, accuracy:0.75\n",
      "ephoch:50,batch:10, accuracy:0.78125\n",
      "ephoch:50,batch:11, accuracy:0.8125\n",
      "ephoch:50,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:51,batch:1, accuracy:0.8125\n",
      "ephoch:51,batch:2, accuracy:0.8125\n",
      "ephoch:51,batch:3, accuracy:0.75\n",
      "ephoch:51,batch:4, accuracy:0.90625\n",
      "ephoch:51,batch:5, accuracy:0.75\n",
      "ephoch:51,batch:6, accuracy:0.75\n",
      "ephoch:51,batch:7, accuracy:0.75\n",
      "ephoch:51,batch:8, accuracy:0.8125\n",
      "ephoch:51,batch:9, accuracy:0.75\n",
      "ephoch:51,batch:10, accuracy:0.78125\n",
      "ephoch:51,batch:11, accuracy:0.8125\n",
      "ephoch:51,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:52,batch:1, accuracy:0.8125\n",
      "ephoch:52,batch:2, accuracy:0.84375\n",
      "ephoch:52,batch:3, accuracy:0.75\n",
      "ephoch:52,batch:4, accuracy:0.90625\n",
      "ephoch:52,batch:5, accuracy:0.75\n",
      "ephoch:52,batch:6, accuracy:0.75\n",
      "ephoch:52,batch:7, accuracy:0.78125\n",
      "ephoch:52,batch:8, accuracy:0.8125\n",
      "ephoch:52,batch:9, accuracy:0.75\n",
      "ephoch:52,batch:10, accuracy:0.78125\n",
      "ephoch:52,batch:11, accuracy:0.8125\n",
      "ephoch:52,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:53,batch:1, accuracy:0.8125\n",
      "ephoch:53,batch:2, accuracy:0.84375\n",
      "ephoch:53,batch:3, accuracy:0.75\n",
      "ephoch:53,batch:4, accuracy:0.90625\n",
      "ephoch:53,batch:5, accuracy:0.75\n",
      "ephoch:53,batch:6, accuracy:0.75\n",
      "ephoch:53,batch:7, accuracy:0.78125\n",
      "ephoch:53,batch:8, accuracy:0.8125\n",
      "ephoch:53,batch:9, accuracy:0.75\n",
      "ephoch:53,batch:10, accuracy:0.78125\n",
      "ephoch:53,batch:11, accuracy:0.8125\n",
      "ephoch:53,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:54,batch:1, accuracy:0.8125\n",
      "ephoch:54,batch:2, accuracy:0.84375\n",
      "ephoch:54,batch:3, accuracy:0.75\n",
      "ephoch:54,batch:4, accuracy:0.90625\n",
      "ephoch:54,batch:5, accuracy:0.75\n",
      "ephoch:54,batch:6, accuracy:0.75\n",
      "ephoch:54,batch:7, accuracy:0.78125\n",
      "ephoch:54,batch:8, accuracy:0.8125\n",
      "ephoch:54,batch:9, accuracy:0.75\n",
      "ephoch:54,batch:10, accuracy:0.78125\n",
      "ephoch:54,batch:11, accuracy:0.8125\n",
      "ephoch:54,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:55,batch:1, accuracy:0.8125\n",
      "ephoch:55,batch:2, accuracy:0.875\n",
      "ephoch:55,batch:3, accuracy:0.75\n",
      "ephoch:55,batch:4, accuracy:0.90625\n",
      "ephoch:55,batch:5, accuracy:0.75\n",
      "ephoch:55,batch:6, accuracy:0.75\n",
      "ephoch:55,batch:7, accuracy:0.78125\n",
      "ephoch:55,batch:8, accuracy:0.8125\n",
      "ephoch:55,batch:9, accuracy:0.75\n",
      "ephoch:55,batch:10, accuracy:0.78125\n",
      "ephoch:55,batch:11, accuracy:0.8125\n",
      "ephoch:55,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:56,batch:1, accuracy:0.8125\n",
      "ephoch:56,batch:2, accuracy:0.875\n",
      "ephoch:56,batch:3, accuracy:0.75\n",
      "ephoch:56,batch:4, accuracy:0.90625\n",
      "ephoch:56,batch:5, accuracy:0.75\n",
      "ephoch:56,batch:6, accuracy:0.75\n",
      "ephoch:56,batch:7, accuracy:0.78125\n",
      "ephoch:56,batch:8, accuracy:0.8125\n",
      "ephoch:56,batch:9, accuracy:0.75\n",
      "ephoch:56,batch:10, accuracy:0.78125\n",
      "ephoch:56,batch:11, accuracy:0.8125\n",
      "ephoch:56,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:57,batch:1, accuracy:0.8125\n",
      "ephoch:57,batch:2, accuracy:0.875\n",
      "ephoch:57,batch:3, accuracy:0.75\n",
      "ephoch:57,batch:4, accuracy:0.90625\n",
      "ephoch:57,batch:5, accuracy:0.75\n",
      "ephoch:57,batch:6, accuracy:0.75\n",
      "ephoch:57,batch:7, accuracy:0.78125\n",
      "ephoch:57,batch:8, accuracy:0.8125\n",
      "ephoch:57,batch:9, accuracy:0.75\n",
      "ephoch:57,batch:10, accuracy:0.78125\n",
      "ephoch:57,batch:11, accuracy:0.8125\n",
      "ephoch:57,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:58,batch:1, accuracy:0.8125\n",
      "ephoch:58,batch:2, accuracy:0.875\n",
      "ephoch:58,batch:3, accuracy:0.75\n",
      "ephoch:58,batch:4, accuracy:0.90625\n",
      "ephoch:58,batch:5, accuracy:0.75\n",
      "ephoch:58,batch:6, accuracy:0.75\n",
      "ephoch:58,batch:7, accuracy:0.78125\n",
      "ephoch:58,batch:8, accuracy:0.8125\n",
      "ephoch:58,batch:9, accuracy:0.75\n",
      "ephoch:58,batch:10, accuracy:0.78125\n",
      "ephoch:58,batch:11, accuracy:0.8125\n",
      "ephoch:58,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:59,batch:1, accuracy:0.8125\n",
      "ephoch:59,batch:2, accuracy:0.875\n",
      "ephoch:59,batch:3, accuracy:0.75\n",
      "ephoch:59,batch:4, accuracy:0.90625\n",
      "ephoch:59,batch:5, accuracy:0.75\n",
      "ephoch:59,batch:6, accuracy:0.75\n",
      "ephoch:59,batch:7, accuracy:0.78125\n",
      "ephoch:59,batch:8, accuracy:0.8125\n",
      "ephoch:59,batch:9, accuracy:0.75\n",
      "ephoch:59,batch:10, accuracy:0.78125\n",
      "ephoch:59,batch:11, accuracy:0.8125\n",
      "ephoch:59,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:60,batch:1, accuracy:0.8125\n",
      "ephoch:60,batch:2, accuracy:0.90625\n",
      "ephoch:60,batch:3, accuracy:0.75\n",
      "ephoch:60,batch:4, accuracy:0.90625\n",
      "ephoch:60,batch:5, accuracy:0.75\n",
      "ephoch:60,batch:6, accuracy:0.75\n",
      "ephoch:60,batch:7, accuracy:0.78125\n",
      "ephoch:60,batch:8, accuracy:0.8125\n",
      "ephoch:60,batch:9, accuracy:0.75\n",
      "ephoch:60,batch:10, accuracy:0.78125\n",
      "ephoch:60,batch:11, accuracy:0.8125\n",
      "ephoch:60,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:61,batch:1, accuracy:0.8125\n",
      "ephoch:61,batch:2, accuracy:0.90625\n",
      "ephoch:61,batch:3, accuracy:0.75\n",
      "ephoch:61,batch:4, accuracy:0.90625\n",
      "ephoch:61,batch:5, accuracy:0.75\n",
      "ephoch:61,batch:6, accuracy:0.75\n",
      "ephoch:61,batch:7, accuracy:0.78125\n",
      "ephoch:61,batch:8, accuracy:0.8125\n",
      "ephoch:61,batch:9, accuracy:0.75\n",
      "ephoch:61,batch:10, accuracy:0.78125\n",
      "ephoch:61,batch:11, accuracy:0.8125\n",
      "ephoch:61,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:62,batch:1, accuracy:0.8125\n",
      "ephoch:62,batch:2, accuracy:0.90625\n",
      "ephoch:62,batch:3, accuracy:0.75\n",
      "ephoch:62,batch:4, accuracy:0.90625\n",
      "ephoch:62,batch:5, accuracy:0.78125\n",
      "ephoch:62,batch:6, accuracy:0.75\n",
      "ephoch:62,batch:7, accuracy:0.78125\n",
      "ephoch:62,batch:8, accuracy:0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ephoch:62,batch:9, accuracy:0.75\n",
      "ephoch:62,batch:10, accuracy:0.78125\n",
      "ephoch:62,batch:11, accuracy:0.8125\n",
      "ephoch:62,batch:12, accuracy:0.7647058963775635\n",
      "ephoch:63,batch:1, accuracy:0.8125\n",
      "ephoch:63,batch:2, accuracy:0.90625\n",
      "ephoch:63,batch:3, accuracy:0.75\n",
      "ephoch:63,batch:4, accuracy:0.90625\n",
      "ephoch:63,batch:5, accuracy:0.78125\n",
      "ephoch:63,batch:6, accuracy:0.75\n",
      "ephoch:63,batch:7, accuracy:0.78125\n",
      "ephoch:63,batch:8, accuracy:0.8125\n",
      "ephoch:63,batch:9, accuracy:0.75\n",
      "ephoch:63,batch:10, accuracy:0.8125\n",
      "ephoch:63,batch:11, accuracy:0.8125\n",
      "ephoch:63,batch:12, accuracy:0.8235294222831726\n",
      "ephoch:64,batch:1, accuracy:0.8125\n",
      "ephoch:64,batch:2, accuracy:0.90625\n",
      "ephoch:64,batch:3, accuracy:0.75\n",
      "ephoch:64,batch:4, accuracy:0.90625\n",
      "ephoch:64,batch:5, accuracy:0.78125\n",
      "ephoch:64,batch:6, accuracy:0.75\n",
      "ephoch:64,batch:7, accuracy:0.78125\n",
      "ephoch:64,batch:8, accuracy:0.8125\n",
      "ephoch:64,batch:9, accuracy:0.75\n",
      "ephoch:64,batch:10, accuracy:0.8125\n",
      "ephoch:64,batch:11, accuracy:0.8125\n",
      "ephoch:64,batch:12, accuracy:0.8235294222831726\n",
      "ephoch:65,batch:1, accuracy:0.8125\n",
      "ephoch:65,batch:2, accuracy:0.90625\n",
      "ephoch:65,batch:3, accuracy:0.75\n",
      "ephoch:65,batch:4, accuracy:0.90625\n",
      "ephoch:65,batch:5, accuracy:0.8125\n",
      "ephoch:65,batch:6, accuracy:0.75\n",
      "ephoch:65,batch:7, accuracy:0.78125\n",
      "ephoch:65,batch:8, accuracy:0.8125\n",
      "ephoch:65,batch:9, accuracy:0.75\n",
      "ephoch:65,batch:10, accuracy:0.8125\n",
      "ephoch:65,batch:11, accuracy:0.8125\n",
      "ephoch:65,batch:12, accuracy:0.8235294222831726\n",
      "ephoch:66,batch:1, accuracy:0.8125\n",
      "ephoch:66,batch:2, accuracy:0.90625\n",
      "ephoch:66,batch:3, accuracy:0.75\n",
      "ephoch:66,batch:4, accuracy:0.90625\n",
      "ephoch:66,batch:5, accuracy:0.8125\n",
      "ephoch:66,batch:6, accuracy:0.75\n",
      "ephoch:66,batch:7, accuracy:0.78125\n",
      "ephoch:66,batch:8, accuracy:0.8125\n",
      "ephoch:66,batch:9, accuracy:0.75\n",
      "ephoch:66,batch:10, accuracy:0.8125\n",
      "ephoch:66,batch:11, accuracy:0.8125\n",
      "ephoch:66,batch:12, accuracy:0.8235294222831726\n",
      "ephoch:67,batch:1, accuracy:0.8125\n",
      "ephoch:67,batch:2, accuracy:0.90625\n",
      "ephoch:67,batch:3, accuracy:0.75\n",
      "ephoch:67,batch:4, accuracy:0.90625\n",
      "ephoch:67,batch:5, accuracy:0.8125\n",
      "ephoch:67,batch:6, accuracy:0.75\n",
      "ephoch:67,batch:7, accuracy:0.78125\n",
      "ephoch:67,batch:8, accuracy:0.84375\n",
      "ephoch:67,batch:9, accuracy:0.75\n",
      "ephoch:67,batch:10, accuracy:0.8125\n",
      "ephoch:67,batch:11, accuracy:0.8125\n",
      "ephoch:67,batch:12, accuracy:0.8235294222831726\n",
      "ephoch:68,batch:1, accuracy:0.8125\n",
      "ephoch:68,batch:2, accuracy:0.90625\n",
      "ephoch:68,batch:3, accuracy:0.75\n",
      "ephoch:68,batch:4, accuracy:0.90625\n",
      "ephoch:68,batch:5, accuracy:0.8125\n",
      "ephoch:68,batch:6, accuracy:0.75\n",
      "ephoch:68,batch:7, accuracy:0.78125\n",
      "ephoch:68,batch:8, accuracy:0.84375\n",
      "ephoch:68,batch:9, accuracy:0.75\n",
      "ephoch:68,batch:10, accuracy:0.8125\n",
      "ephoch:68,batch:11, accuracy:0.8125\n",
      "ephoch:68,batch:12, accuracy:0.8235294222831726\n",
      "ephoch:69,batch:1, accuracy:0.8125\n",
      "ephoch:69,batch:2, accuracy:0.90625\n",
      "ephoch:69,batch:3, accuracy:0.75\n",
      "ephoch:69,batch:4, accuracy:0.90625\n",
      "ephoch:69,batch:5, accuracy:0.8125\n",
      "ephoch:69,batch:6, accuracy:0.75\n",
      "ephoch:69,batch:7, accuracy:0.78125\n",
      "ephoch:69,batch:8, accuracy:0.84375\n",
      "ephoch:69,batch:9, accuracy:0.75\n",
      "ephoch:69,batch:10, accuracy:0.8125\n",
      "ephoch:69,batch:11, accuracy:0.8125\n",
      "ephoch:69,batch:12, accuracy:0.8235294222831726\n",
      "ephoch:70,batch:1, accuracy:0.8125\n",
      "ephoch:70,batch:2, accuracy:0.90625\n",
      "ephoch:70,batch:3, accuracy:0.75\n",
      "ephoch:70,batch:4, accuracy:0.90625\n",
      "ephoch:70,batch:5, accuracy:0.8125\n",
      "ephoch:70,batch:6, accuracy:0.75\n",
      "ephoch:70,batch:7, accuracy:0.78125\n",
      "ephoch:70,batch:8, accuracy:0.84375\n",
      "ephoch:70,batch:9, accuracy:0.75\n",
      "ephoch:70,batch:10, accuracy:0.8125\n",
      "ephoch:70,batch:11, accuracy:0.8125\n",
      "ephoch:70,batch:12, accuracy:0.8235294222831726\n",
      "ephoch:71,batch:1, accuracy:0.8125\n",
      "ephoch:71,batch:2, accuracy:0.9375\n",
      "ephoch:71,batch:3, accuracy:0.78125\n",
      "ephoch:71,batch:4, accuracy:0.90625\n",
      "ephoch:71,batch:5, accuracy:0.8125\n",
      "ephoch:71,batch:6, accuracy:0.75\n",
      "ephoch:71,batch:7, accuracy:0.78125\n",
      "ephoch:71,batch:8, accuracy:0.84375\n",
      "ephoch:71,batch:9, accuracy:0.75\n",
      "ephoch:71,batch:10, accuracy:0.8125\n",
      "ephoch:71,batch:11, accuracy:0.8125\n",
      "ephoch:71,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:72,batch:1, accuracy:0.8125\n",
      "ephoch:72,batch:2, accuracy:0.9375\n",
      "ephoch:72,batch:3, accuracy:0.78125\n",
      "ephoch:72,batch:4, accuracy:0.90625\n",
      "ephoch:72,batch:5, accuracy:0.8125\n",
      "ephoch:72,batch:6, accuracy:0.75\n",
      "ephoch:72,batch:7, accuracy:0.78125\n",
      "ephoch:72,batch:8, accuracy:0.84375\n",
      "ephoch:72,batch:9, accuracy:0.75\n",
      "ephoch:72,batch:10, accuracy:0.8125\n",
      "ephoch:72,batch:11, accuracy:0.8125\n",
      "ephoch:72,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:73,batch:1, accuracy:0.8125\n",
      "ephoch:73,batch:2, accuracy:0.9375\n",
      "ephoch:73,batch:3, accuracy:0.78125\n",
      "ephoch:73,batch:4, accuracy:0.90625\n",
      "ephoch:73,batch:5, accuracy:0.8125\n",
      "ephoch:73,batch:6, accuracy:0.75\n",
      "ephoch:73,batch:7, accuracy:0.78125\n",
      "ephoch:73,batch:8, accuracy:0.84375\n",
      "ephoch:73,batch:9, accuracy:0.75\n",
      "ephoch:73,batch:10, accuracy:0.8125\n",
      "ephoch:73,batch:11, accuracy:0.8125\n",
      "ephoch:73,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:74,batch:1, accuracy:0.8125\n",
      "ephoch:74,batch:2, accuracy:0.9375\n",
      "ephoch:74,batch:3, accuracy:0.8125\n",
      "ephoch:74,batch:4, accuracy:0.90625\n",
      "ephoch:74,batch:5, accuracy:0.8125\n",
      "ephoch:74,batch:6, accuracy:0.75\n",
      "ephoch:74,batch:7, accuracy:0.78125\n",
      "ephoch:74,batch:8, accuracy:0.84375\n",
      "ephoch:74,batch:9, accuracy:0.75\n",
      "ephoch:74,batch:10, accuracy:0.8125\n",
      "ephoch:74,batch:11, accuracy:0.8125\n",
      "ephoch:74,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:75,batch:1, accuracy:0.8125\n",
      "ephoch:75,batch:2, accuracy:0.9375\n",
      "ephoch:75,batch:3, accuracy:0.8125\n",
      "ephoch:75,batch:4, accuracy:0.90625\n",
      "ephoch:75,batch:5, accuracy:0.8125\n",
      "ephoch:75,batch:6, accuracy:0.75\n",
      "ephoch:75,batch:7, accuracy:0.78125\n",
      "ephoch:75,batch:8, accuracy:0.84375\n",
      "ephoch:75,batch:9, accuracy:0.75\n",
      "ephoch:75,batch:10, accuracy:0.8125\n",
      "ephoch:75,batch:11, accuracy:0.8125\n",
      "ephoch:75,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:76,batch:1, accuracy:0.8125\n",
      "ephoch:76,batch:2, accuracy:0.9375\n",
      "ephoch:76,batch:3, accuracy:0.8125\n",
      "ephoch:76,batch:4, accuracy:0.90625\n",
      "ephoch:76,batch:5, accuracy:0.8125\n",
      "ephoch:76,batch:6, accuracy:0.75\n",
      "ephoch:76,batch:7, accuracy:0.78125\n",
      "ephoch:76,batch:8, accuracy:0.84375\n",
      "ephoch:76,batch:9, accuracy:0.75\n",
      "ephoch:76,batch:10, accuracy:0.8125\n",
      "ephoch:76,batch:11, accuracy:0.8125\n",
      "ephoch:76,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:77,batch:1, accuracy:0.8125\n",
      "ephoch:77,batch:2, accuracy:0.9375\n",
      "ephoch:77,batch:3, accuracy:0.8125\n",
      "ephoch:77,batch:4, accuracy:0.90625\n",
      "ephoch:77,batch:5, accuracy:0.8125\n",
      "ephoch:77,batch:6, accuracy:0.75\n",
      "ephoch:77,batch:7, accuracy:0.78125\n",
      "ephoch:77,batch:8, accuracy:0.84375\n",
      "ephoch:77,batch:9, accuracy:0.75\n",
      "ephoch:77,batch:10, accuracy:0.84375\n",
      "ephoch:77,batch:11, accuracy:0.8125\n",
      "ephoch:77,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:78,batch:1, accuracy:0.8125\n",
      "ephoch:78,batch:2, accuracy:0.9375\n",
      "ephoch:78,batch:3, accuracy:0.8125\n",
      "ephoch:78,batch:4, accuracy:0.90625\n",
      "ephoch:78,batch:5, accuracy:0.8125\n",
      "ephoch:78,batch:6, accuracy:0.75\n",
      "ephoch:78,batch:7, accuracy:0.78125\n",
      "ephoch:78,batch:8, accuracy:0.84375\n",
      "ephoch:78,batch:9, accuracy:0.75\n",
      "ephoch:78,batch:10, accuracy:0.84375\n",
      "ephoch:78,batch:11, accuracy:0.84375\n",
      "ephoch:78,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:79,batch:1, accuracy:0.8125\n",
      "ephoch:79,batch:2, accuracy:0.9375\n",
      "ephoch:79,batch:3, accuracy:0.8125\n",
      "ephoch:79,batch:4, accuracy:0.90625\n",
      "ephoch:79,batch:5, accuracy:0.8125\n",
      "ephoch:79,batch:6, accuracy:0.78125\n",
      "ephoch:79,batch:7, accuracy:0.78125\n",
      "ephoch:79,batch:8, accuracy:0.84375\n",
      "ephoch:79,batch:9, accuracy:0.75\n",
      "ephoch:79,batch:10, accuracy:0.84375\n",
      "ephoch:79,batch:11, accuracy:0.84375\n",
      "ephoch:79,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:80,batch:1, accuracy:0.8125\n",
      "ephoch:80,batch:2, accuracy:0.9375\n",
      "ephoch:80,batch:3, accuracy:0.8125\n",
      "ephoch:80,batch:4, accuracy:0.90625\n",
      "ephoch:80,batch:5, accuracy:0.8125\n",
      "ephoch:80,batch:6, accuracy:0.78125\n",
      "ephoch:80,batch:7, accuracy:0.78125\n",
      "ephoch:80,batch:8, accuracy:0.84375\n",
      "ephoch:80,batch:9, accuracy:0.75\n",
      "ephoch:80,batch:10, accuracy:0.84375\n",
      "ephoch:80,batch:11, accuracy:0.84375\n",
      "ephoch:80,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:81,batch:1, accuracy:0.8125\n",
      "ephoch:81,batch:2, accuracy:0.9375\n",
      "ephoch:81,batch:3, accuracy:0.8125\n",
      "ephoch:81,batch:4, accuracy:0.90625\n",
      "ephoch:81,batch:5, accuracy:0.8125\n",
      "ephoch:81,batch:6, accuracy:0.78125\n",
      "ephoch:81,batch:7, accuracy:0.8125\n",
      "ephoch:81,batch:8, accuracy:0.84375\n",
      "ephoch:81,batch:9, accuracy:0.75\n",
      "ephoch:81,batch:10, accuracy:0.84375\n",
      "ephoch:81,batch:11, accuracy:0.84375\n",
      "ephoch:81,batch:12, accuracy:0.8823529481887817\n",
      "ephoch:82,batch:1, accuracy:0.8125\n",
      "ephoch:82,batch:2, accuracy:0.9375\n",
      "ephoch:82,batch:3, accuracy:0.8125\n",
      "ephoch:82,batch:4, accuracy:0.90625\n",
      "ephoch:82,batch:5, accuracy:0.8125\n",
      "ephoch:82,batch:6, accuracy:0.78125\n",
      "ephoch:82,batch:7, accuracy:0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ephoch:82,batch:8, accuracy:0.84375\n",
      "ephoch:82,batch:9, accuracy:0.75\n",
      "ephoch:82,batch:10, accuracy:0.84375\n",
      "ephoch:82,batch:11, accuracy:0.84375\n",
      "ephoch:82,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:83,batch:1, accuracy:0.8125\n",
      "ephoch:83,batch:2, accuracy:0.9375\n",
      "ephoch:83,batch:3, accuracy:0.8125\n",
      "ephoch:83,batch:4, accuracy:0.90625\n",
      "ephoch:83,batch:5, accuracy:0.8125\n",
      "ephoch:83,batch:6, accuracy:0.78125\n",
      "ephoch:83,batch:7, accuracy:0.8125\n",
      "ephoch:83,batch:8, accuracy:0.84375\n",
      "ephoch:83,batch:9, accuracy:0.75\n",
      "ephoch:83,batch:10, accuracy:0.875\n",
      "ephoch:83,batch:11, accuracy:0.84375\n",
      "ephoch:83,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:84,batch:1, accuracy:0.8125\n",
      "ephoch:84,batch:2, accuracy:0.9375\n",
      "ephoch:84,batch:3, accuracy:0.8125\n",
      "ephoch:84,batch:4, accuracy:0.90625\n",
      "ephoch:84,batch:5, accuracy:0.8125\n",
      "ephoch:84,batch:6, accuracy:0.78125\n",
      "ephoch:84,batch:7, accuracy:0.8125\n",
      "ephoch:84,batch:8, accuracy:0.875\n",
      "ephoch:84,batch:9, accuracy:0.75\n",
      "ephoch:84,batch:10, accuracy:0.875\n",
      "ephoch:84,batch:11, accuracy:0.84375\n",
      "ephoch:84,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:85,batch:1, accuracy:0.8125\n",
      "ephoch:85,batch:2, accuracy:0.9375\n",
      "ephoch:85,batch:3, accuracy:0.8125\n",
      "ephoch:85,batch:4, accuracy:0.90625\n",
      "ephoch:85,batch:5, accuracy:0.8125\n",
      "ephoch:85,batch:6, accuracy:0.78125\n",
      "ephoch:85,batch:7, accuracy:0.8125\n",
      "ephoch:85,batch:8, accuracy:0.875\n",
      "ephoch:85,batch:9, accuracy:0.75\n",
      "ephoch:85,batch:10, accuracy:0.875\n",
      "ephoch:85,batch:11, accuracy:0.84375\n",
      "ephoch:85,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:86,batch:1, accuracy:0.8125\n",
      "ephoch:86,batch:2, accuracy:0.9375\n",
      "ephoch:86,batch:3, accuracy:0.84375\n",
      "ephoch:86,batch:4, accuracy:0.90625\n",
      "ephoch:86,batch:5, accuracy:0.8125\n",
      "ephoch:86,batch:6, accuracy:0.78125\n",
      "ephoch:86,batch:7, accuracy:0.8125\n",
      "ephoch:86,batch:8, accuracy:0.875\n",
      "ephoch:86,batch:9, accuracy:0.75\n",
      "ephoch:86,batch:10, accuracy:0.875\n",
      "ephoch:86,batch:11, accuracy:0.84375\n",
      "ephoch:86,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:87,batch:1, accuracy:0.8125\n",
      "ephoch:87,batch:2, accuracy:0.9375\n",
      "ephoch:87,batch:3, accuracy:0.84375\n",
      "ephoch:87,batch:4, accuracy:0.90625\n",
      "ephoch:87,batch:5, accuracy:0.8125\n",
      "ephoch:87,batch:6, accuracy:0.78125\n",
      "ephoch:87,batch:7, accuracy:0.8125\n",
      "ephoch:87,batch:8, accuracy:0.875\n",
      "ephoch:87,batch:9, accuracy:0.75\n",
      "ephoch:87,batch:10, accuracy:0.875\n",
      "ephoch:87,batch:11, accuracy:0.84375\n",
      "ephoch:87,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:88,batch:1, accuracy:0.8125\n",
      "ephoch:88,batch:2, accuracy:0.9375\n",
      "ephoch:88,batch:3, accuracy:0.84375\n",
      "ephoch:88,batch:4, accuracy:0.90625\n",
      "ephoch:88,batch:5, accuracy:0.8125\n",
      "ephoch:88,batch:6, accuracy:0.78125\n",
      "ephoch:88,batch:7, accuracy:0.8125\n",
      "ephoch:88,batch:8, accuracy:0.875\n",
      "ephoch:88,batch:9, accuracy:0.75\n",
      "ephoch:88,batch:10, accuracy:0.875\n",
      "ephoch:88,batch:11, accuracy:0.84375\n",
      "ephoch:88,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:89,batch:1, accuracy:0.8125\n",
      "ephoch:89,batch:2, accuracy:0.9375\n",
      "ephoch:89,batch:3, accuracy:0.84375\n",
      "ephoch:89,batch:4, accuracy:0.90625\n",
      "ephoch:89,batch:5, accuracy:0.8125\n",
      "ephoch:89,batch:6, accuracy:0.78125\n",
      "ephoch:89,batch:7, accuracy:0.8125\n",
      "ephoch:89,batch:8, accuracy:0.875\n",
      "ephoch:89,batch:9, accuracy:0.75\n",
      "ephoch:89,batch:10, accuracy:0.875\n",
      "ephoch:89,batch:11, accuracy:0.84375\n",
      "ephoch:89,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:90,batch:1, accuracy:0.8125\n",
      "ephoch:90,batch:2, accuracy:0.9375\n",
      "ephoch:90,batch:3, accuracy:0.84375\n",
      "ephoch:90,batch:4, accuracy:0.90625\n",
      "ephoch:90,batch:5, accuracy:0.8125\n",
      "ephoch:90,batch:6, accuracy:0.78125\n",
      "ephoch:90,batch:7, accuracy:0.8125\n",
      "ephoch:90,batch:8, accuracy:0.875\n",
      "ephoch:90,batch:9, accuracy:0.75\n",
      "ephoch:90,batch:10, accuracy:0.875\n",
      "ephoch:90,batch:11, accuracy:0.84375\n",
      "ephoch:90,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:91,batch:1, accuracy:0.8125\n",
      "ephoch:91,batch:2, accuracy:0.9375\n",
      "ephoch:91,batch:3, accuracy:0.84375\n",
      "ephoch:91,batch:4, accuracy:0.90625\n",
      "ephoch:91,batch:5, accuracy:0.8125\n",
      "ephoch:91,batch:6, accuracy:0.78125\n",
      "ephoch:91,batch:7, accuracy:0.8125\n",
      "ephoch:91,batch:8, accuracy:0.875\n",
      "ephoch:91,batch:9, accuracy:0.75\n",
      "ephoch:91,batch:10, accuracy:0.875\n",
      "ephoch:91,batch:11, accuracy:0.84375\n",
      "ephoch:91,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:92,batch:1, accuracy:0.8125\n",
      "ephoch:92,batch:2, accuracy:0.9375\n",
      "ephoch:92,batch:3, accuracy:0.84375\n",
      "ephoch:92,batch:4, accuracy:0.90625\n",
      "ephoch:92,batch:5, accuracy:0.8125\n",
      "ephoch:92,batch:6, accuracy:0.78125\n",
      "ephoch:92,batch:7, accuracy:0.8125\n",
      "ephoch:92,batch:8, accuracy:0.875\n",
      "ephoch:92,batch:9, accuracy:0.75\n",
      "ephoch:92,batch:10, accuracy:0.875\n",
      "ephoch:92,batch:11, accuracy:0.84375\n",
      "ephoch:92,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:93,batch:1, accuracy:0.8125\n",
      "ephoch:93,batch:2, accuracy:0.9375\n",
      "ephoch:93,batch:3, accuracy:0.84375\n",
      "ephoch:93,batch:4, accuracy:0.90625\n",
      "ephoch:93,batch:5, accuracy:0.8125\n",
      "ephoch:93,batch:6, accuracy:0.78125\n",
      "ephoch:93,batch:7, accuracy:0.84375\n",
      "ephoch:93,batch:8, accuracy:0.875\n",
      "ephoch:93,batch:9, accuracy:0.75\n",
      "ephoch:93,batch:10, accuracy:0.875\n",
      "ephoch:93,batch:11, accuracy:0.84375\n",
      "ephoch:93,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:94,batch:1, accuracy:0.8125\n",
      "ephoch:94,batch:2, accuracy:0.9375\n",
      "ephoch:94,batch:3, accuracy:0.84375\n",
      "ephoch:94,batch:4, accuracy:0.90625\n",
      "ephoch:94,batch:5, accuracy:0.8125\n",
      "ephoch:94,batch:6, accuracy:0.78125\n",
      "ephoch:94,batch:7, accuracy:0.84375\n",
      "ephoch:94,batch:8, accuracy:0.875\n",
      "ephoch:94,batch:9, accuracy:0.75\n",
      "ephoch:94,batch:10, accuracy:0.875\n",
      "ephoch:94,batch:11, accuracy:0.84375\n",
      "ephoch:94,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:95,batch:1, accuracy:0.8125\n",
      "ephoch:95,batch:2, accuracy:0.9375\n",
      "ephoch:95,batch:3, accuracy:0.84375\n",
      "ephoch:95,batch:4, accuracy:0.90625\n",
      "ephoch:95,batch:5, accuracy:0.84375\n",
      "ephoch:95,batch:6, accuracy:0.78125\n",
      "ephoch:95,batch:7, accuracy:0.84375\n",
      "ephoch:95,batch:8, accuracy:0.875\n",
      "ephoch:95,batch:9, accuracy:0.75\n",
      "ephoch:95,batch:10, accuracy:0.875\n",
      "ephoch:95,batch:11, accuracy:0.84375\n",
      "ephoch:95,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:96,batch:1, accuracy:0.8125\n",
      "ephoch:96,batch:2, accuracy:0.9375\n",
      "ephoch:96,batch:3, accuracy:0.84375\n",
      "ephoch:96,batch:4, accuracy:0.90625\n",
      "ephoch:96,batch:5, accuracy:0.84375\n",
      "ephoch:96,batch:6, accuracy:0.78125\n",
      "ephoch:96,batch:7, accuracy:0.84375\n",
      "ephoch:96,batch:8, accuracy:0.875\n",
      "ephoch:96,batch:9, accuracy:0.75\n",
      "ephoch:96,batch:10, accuracy:0.875\n",
      "ephoch:96,batch:11, accuracy:0.84375\n",
      "ephoch:96,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:97,batch:1, accuracy:0.8125\n",
      "ephoch:97,batch:2, accuracy:0.9375\n",
      "ephoch:97,batch:3, accuracy:0.84375\n",
      "ephoch:97,batch:4, accuracy:0.90625\n",
      "ephoch:97,batch:5, accuracy:0.84375\n",
      "ephoch:97,batch:6, accuracy:0.78125\n",
      "ephoch:97,batch:7, accuracy:0.84375\n",
      "ephoch:97,batch:8, accuracy:0.875\n",
      "ephoch:97,batch:9, accuracy:0.75\n",
      "ephoch:97,batch:10, accuracy:0.875\n",
      "ephoch:97,batch:11, accuracy:0.84375\n",
      "ephoch:97,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:98,batch:1, accuracy:0.8125\n",
      "ephoch:98,batch:2, accuracy:0.9375\n",
      "ephoch:98,batch:3, accuracy:0.84375\n",
      "ephoch:98,batch:4, accuracy:0.90625\n",
      "ephoch:98,batch:5, accuracy:0.84375\n",
      "ephoch:98,batch:6, accuracy:0.78125\n",
      "ephoch:98,batch:7, accuracy:0.84375\n",
      "ephoch:98,batch:8, accuracy:0.875\n",
      "ephoch:98,batch:9, accuracy:0.75\n",
      "ephoch:98,batch:10, accuracy:0.875\n",
      "ephoch:98,batch:11, accuracy:0.84375\n",
      "ephoch:98,batch:12, accuracy:0.9411764740943909\n",
      "ephoch:99,batch:1, accuracy:0.8125\n",
      "ephoch:99,batch:2, accuracy:0.9375\n",
      "ephoch:99,batch:3, accuracy:0.84375\n",
      "ephoch:99,batch:4, accuracy:0.90625\n",
      "ephoch:99,batch:5, accuracy:0.84375\n",
      "ephoch:99,batch:6, accuracy:0.78125\n",
      "ephoch:99,batch:7, accuracy:0.84375\n",
      "ephoch:99,batch:8, accuracy:0.875\n",
      "ephoch:99,batch:9, accuracy:0.75\n",
      "ephoch:99,batch:10, accuracy:0.875\n",
      "ephoch:99,batch:11, accuracy:0.84375\n",
      "ephoch:99,batch:12, accuracy:0.9411764740943909\n",
      "accuracy test: 0.731183\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\t\n",
    "    for i in range(n_epochs):\n",
    "        m =0\n",
    "        for feature, label in batch_generator(feature_train,labels_train, batch_size): \n",
    "            _, loss_batch, accuracy_batch = sess.run([optimizer, loss, accuracy], feed_dict={features:feature, labels: label, isTraining: True})\n",
    "            m +=1\n",
    "            print('ephoch:{0},batch:{1}, accuracy:{2}' .format(i, m, accuracy_batch))\n",
    "            \n",
    "    accuracy_test = sess.run(accuracy, feed_dict={features:feature_test, labels: labels_test, isTraining: True})\n",
    "    print('accuracy test:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### sandard scaler does not Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
