{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist_image_shape = [28, 28, 1]\n",
    "\n",
    "def load_dataset():\n",
    "    return input_data.read_data_sets('./data/MNIST_data')\n",
    "\n",
    "def get_next_batch(dataset, batch_size):\n",
    "    # dataset should be mnist.(train/val/test)\n",
    "    batch, _ = dataset.next_batch(batch_size)\n",
    "    batch_shape = [batch_size] + mnist_image_shape\n",
    "    return np.reshape(batch, batch_shape)\n",
    "\n",
    "def visualize(_original, _reconstructions, num_visualize):\n",
    "    vis_folder = './vis/'\n",
    "    if not os.path.exists(vis_folder):\n",
    "          os.makedirs(vis_folder)\n",
    "\n",
    "    original = _original[:num_visualize]\n",
    "    reconstructions = _reconstructions[:num_visualize]\n",
    "    \n",
    "    count = 1\n",
    "    for (orig, rec) in zip(original, reconstructions):\n",
    "        orig = np.reshape(orig, (mnist_image_shape[0],\n",
    "                                 mnist_image_shape[1]))\n",
    "        rec = np.reshape(rec, (mnist_image_shape[0],\n",
    "                               mnist_image_shape[1]))\n",
    "        f, ax = plt.subplots(1,2)\n",
    "        ax[0].imshow(orig, cmap='gray')\n",
    "        ax[1].imshow(rec, cmap='gray')\n",
    "        plt.savefig(vis_folder + \"test_%d.png\" % count)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. layer_utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_deconv2d_output_dims(input_dims, filter_dims, stride_dims, padding):\n",
    "    # Returns the height and width of the output of a deconvolution layer.\n",
    "    batch_size, input_h, input_w, num_channels_in = input_dims\n",
    "    filter_h, filter_w, num_channels_out  = filter_dims\n",
    "    stride_h, stride_w = stride_dims\n",
    "\n",
    "    # Compute the height in the output, based on the padding.\n",
    "    if padding == 'SAME':\n",
    "        out_h = input_h * stride_h\n",
    "    elif padding == 'VALID':\n",
    "        out_h = (input_h - 1) * stride_h + filter_h\n",
    "\n",
    "    # Compute the width in the output, based on the padding.\n",
    "    if padding == 'SAME':\n",
    "        out_w = input_w * stride_w\n",
    "    elif padding == 'VALID':\n",
    "        out_w = (input_w - 1) * stride_w + filter_w\n",
    "\n",
    "    return [batch_size, out_h, out_w, num_channels_out]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "#from layer_utils import get_deconv2d_output_dims\n",
    "\n",
    "def conv(my_input, name, filter_dims, stride_dims, padding='SAME',\n",
    "         non_linear_fn=tf.nn.relu):\n",
    "    input_dims = my_input.get_shape().as_list()\n",
    "    assert(len(input_dims) == 4) # batch_size, height, width, num_channels_in\n",
    "    assert(len(filter_dims) == 3) # height, width and num_channels out\n",
    "    assert(len(stride_dims) == 2) # stride height and width\n",
    "\n",
    "    num_channels_in = input_dims[-1]\n",
    "    filter_h, filter_w, num_channels_out = filter_dims\n",
    "    stride_h, stride_w = stride_dims\n",
    "\n",
    "    # Define a variable scope for the conv layer\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # Create filter weight variable\n",
    "        kernel = tf.get_variable(name='filter', shape=[filter_h, filter_w, num_channels_in, num_channels_out ],\\\n",
    "                                 dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        # Create bias variable\n",
    "        biases = tf.get_variable(name='biases', shape=[num_channels_out],dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "        # Define the convolution flow graph\n",
    "        conv1= tf.nn.conv2d(input=my_input, filter=kernel, strides=[1, stride_h, stride_w, 1], padding=padding)\n",
    "        # Add bias to conv output\n",
    "        conv2 = tf.add(conv1, biases, name = scope.name)\n",
    "        # Apply non-linearity (if asked) and return output\n",
    "        if non_linear_fn != None:\n",
    "            conv2d = non_linear_fn(conv2)\n",
    "            return conv2d\n",
    "        else:\n",
    "            return conv2\n",
    "    \n",
    "def deconv(my_input, name, filter_dims, stride_dims, padding='SAME',\n",
    "           non_linear_fn=tf.nn.relu):\n",
    "    input_dims = my_input.get_shape().as_list()\n",
    "    assert(len(input_dims) == 4) # batch_size, height, width, num_channels_in\n",
    "    assert(len(filter_dims) == 3) # height, width and num_channels out\n",
    "    assert(len(stride_dims) == 2) # stride height and width\n",
    "\n",
    "    num_channels_in = input_dims[-1]\n",
    "    filter_h, filter_w, num_channels_out = filter_dims\n",
    "    stride_h, stride_w = stride_dims\n",
    "    # Let's step into this function\n",
    "    output_dims = get_deconv2d_output_dims(input_dims,\n",
    "                                           filter_dims,\n",
    "                                           stride_dims,\n",
    "                                           padding)\n",
    "\n",
    "    # Define a variable scope for the deconv layer\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # Create filter weight variable\n",
    "        # Note that num_channels_out and in positions are flipped for deconv.\n",
    "        kernel = tf.get_variable(name='filter', shape=[filter_h, filter_w, num_channels_out, num_channels_in ], \\\n",
    "                                 dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        # Create bias variable\n",
    "        biases = tf.get_variable(name='biases', shape=[num_channels_out], dtype=tf.float32,initializer=tf.constant_initializer(0.0))\n",
    "        # Define the deconv flow graph\n",
    "        deconv1= tf.nn.conv2d_transpose(value=my_input, filter= kernel, strides=[1, stride_h, stride_w, 1], \\\n",
    "                                        output_shape= output_dims, padding=padding)\n",
    "        # Add bias to deconv output\n",
    "        deconv2 = tf.add(deconv1, biases, name=scope.name)\n",
    "        # Apply non-linearity (if asked) and return output\n",
    "        if non_linear_fn != None:\n",
    "            deconv2d = non_linear_fn(deconv2)\n",
    "            return deconv2d\n",
    "        else:\n",
    "            return deconv2\n",
    "\n",
    "def max_pool(my_input, name, filter_dims, stride_dims, padding='SAME'):\n",
    "    assert(len(filter_dims) == 2) # filter height and width\n",
    "    assert(len(stride_dims) == 2) # stride height and width\n",
    "\n",
    "    filter_h, filter_w = filter_dims\n",
    "    stride_h, stride_w = stride_dims\n",
    "    \n",
    "    # Define the max pool flow graph and return output\n",
    "    pool =  tf.nn.max_pool(value=my_input, ksize=[1, filter_h, filter_w, 1], strides=[1, stride_h, stride_w, 1],\\\n",
    "                            padding= padding, name= name)\n",
    "    return pool\n",
    "def fc(my_input, name, out_dim, non_linear_fn=tf.nn.relu):\n",
    "    assert(type(out_dim) == int)\n",
    "\n",
    "    # Define a variable scope for the FC layer\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        input_dims = my_input.get_shape().as_list()\n",
    "        # the input to the fc layer should be flattened\n",
    "        if len(input_dims) == 4:\n",
    "            # for eg. the output of a conv layer\n",
    "            batch_size, input_h, input_w, num_channels = input_dims\n",
    "            # ignore the batch dimension\n",
    "            in_dim = input_h * input_w * num_channels\n",
    "            flat_input = tf.reshape(my_input, [batch_size, in_dim])\n",
    "        else:\n",
    "            in_dim = input_dims[-1]\n",
    "            flat_input = my_input\n",
    "\n",
    "        # Create weight variable\n",
    "        weight = tf.get_variable(name='weight', shape=[in_dim, out_dim],dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        # Create bias variable\n",
    "        bias = tf.get_variable(name='bias', shape=[out_dim], dtype=tf.float32,initializer=tf.constant_initializer(0.0))\n",
    "        # Define FC flow graph\n",
    "        fc1 = tf.add(tf.matmul(flat_input, weight), bias, name= scope.name)\n",
    "        # Apply non-linearity (if asked) and return output\n",
    "        if non_linear_fn != None:\n",
    "            fc = non_linear_fn(fc1)\n",
    "            return fc\n",
    "        else:\n",
    "            return fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.autoencoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **original structure never converges, more complicated decoder is used here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "#from layers import *\n",
    "\n",
    "def encoder(my_input):\n",
    "    # Create a conv network with 3 conv layers and 1 FC layer\n",
    "    # Conv 1: filter: [3, 3, 1], stride: [2, 2], relu\n",
    "    conv1 = conv(my_input, 'conv1', [3, 3, 32], [2, 2], padding='SAME',\n",
    "         non_linear_fn=tf.nn.relu)\n",
    "    # Conv 2: filter: [3, 3, 8], stride: [2, 2], relu\n",
    "    conv2 = conv(conv1, 'conv2', [3, 3, 32], [2, 2], padding='SAME',\n",
    "         non_linear_fn=tf.nn.relu)\n",
    "    # Conv 3: filter: [3, 3, 8], stride: [2, 2], relu\n",
    "    conv3 = conv(conv2, 'conv3', [3, 3, 16], [2, 2], padding='SAME',\n",
    "         non_linear_fn=tf.nn.relu)\n",
    "    # FC: output_dim: 100, no non-linearity\n",
    "    fc_encoder = fc(conv3, 'fc_encoder', 128, non_linear_fn=None)\n",
    "    #raise NotImplementedError\n",
    "    return fc_encoder\n",
    "def decoder(my_input):\n",
    "    # Create a deconv network with 1 FC layer and 3 deconv layers\n",
    "    # FC: output dim: 128, relu\n",
    "    fc_decoder = fc(my_input, 'fc_decoder', 144, non_linear_fn=tf.nn.relu)\n",
    "    # Reshape to [batch_size, 4, 4, 8]\n",
    "    reshape = tf.reshape(fc_decoder, [-1, 3, 3,16])\n",
    "    # Deconv 1: filter: [3, 3, 8], stride: [2, 2], relu\n",
    "    deconv1 = deconv(reshape, 'deconv1',  [3, 3, 32], [2, 2], padding='VALID',\n",
    "           non_linear_fn=tf.nn.relu)\n",
    "    # Deconv 2: filter: [8, 8, 1], stride: [2, 2], padding: valid, relu\n",
    "    deconv2 = deconv(deconv1, 'deconv2',  [3, 3, 32], [2, 2], padding='SAME',\n",
    "           non_linear_fn=tf.nn.relu)\n",
    "    # Deconv 3: filter: [7, 7, 1], stride: [1, 1], padding: valid, sigmoid\n",
    "    deconv3 = deconv(deconv2, 'deconv3',  [3, 3, 1], [2, 2], padding='SAME',\n",
    "           non_linear_fn=tf.nn.sigmoid)\n",
    "    #raise NotImplementedError\n",
    "    return deconv3\n",
    "    \n",
    "def autoencoder(input_shape):\n",
    "    # Define place holder with input shape\n",
    "    imag = tf.placeholder(tf.float32, input_shape, name ='imag_in')\n",
    "    # Define variable scope for autoencoder\n",
    "    with tf.variable_scope('autoencoder') as scope:\n",
    "        # Pass input to encoder to obtain encoding\n",
    "        encoding = encoder(imag)\n",
    "        # Pass encoding into decoder to obtain reconstructed image\n",
    "        reconstructed_imag = decoder(encoding)\n",
    "        # Return input image (placeholder) and reconstructed image\n",
    "    return imag, reconstructed_imag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.tain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "#from utils import *\n",
    "#from autoencoder import *\n",
    "\n",
    "batch_size = 256\n",
    "batch_shape = (batch_size, 28, 28, 1)\n",
    "num_visualize = 10\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "def calculate_loss(original, reconstructed):\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(reconstructed, original)), axis=[1,2,3]))\n",
    "\n",
    "def train(dataset):\n",
    "    input_image, reconstructed_image = autoencoder(batch_shape)\n",
    "    loss = calculate_loss(input_image, reconstructed_image)\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        writer = tf.summary.FileWriter('my_graph', session.graph)\n",
    "        writer.close()\n",
    "        session.run(init)\n",
    "        \n",
    "        dataset_size = len(dataset.train.images)\n",
    "        print (\"Dataset size:\", dataset_size)\n",
    "        num_iters = (num_epochs * dataset_size)//batch_size\n",
    "        print (\"Num iters:\", num_iters)\n",
    "        for step in range(num_iters):\n",
    "            input_batch  = get_next_batch(dataset.train, batch_size)\n",
    "            loss_val,  _ = session.run([loss, optimizer], \n",
    "                                       feed_dict={input_image: input_batch})\n",
    "            if step % 1000 == 0:\n",
    "                print (\"Loss at step\", step, \":\", loss_val)\n",
    "\n",
    "        test_batch = get_next_batch(dataset.test, batch_size)\n",
    "        reconstruction = session.run(reconstructed_image,\n",
    "                                     feed_dict={input_image: test_batch})\n",
    "        visualize(test_batch, reconstruction, num_visualize)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Dataset size: 55000\n",
      "Num iters: 10742\n",
      "Loss at step 0 : 181.34\n",
      "Loss at step 1000 : 19.1241\n",
      "Loss at step 2000 : 11.0623\n",
      "Loss at step 3000 : 8.3195\n",
      "Loss at step 4000 : 8.27414\n",
      "Loss at step 5000 : 6.92209\n",
      "Loss at step 6000 : 6.52266\n",
      "Loss at step 7000 : 5.86308\n",
      "Loss at step 8000 : 5.50316\n",
      "Loss at step 9000 : 5.55846\n",
      "Loss at step 10000 : 4.8272\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = load_dataset()\n",
    "train(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
